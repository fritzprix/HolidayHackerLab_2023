{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "config.json: 100%|██████████| 863/863 [00:00<00:00, 2.03MB/s]\n",
      "configuration_phi.py: 100%|██████████| 9.26k/9.26k [00:00<00:00, 6.06MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-2:\n",
      "- configuration_phi.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "modeling_phi.py: 100%|██████████| 62.7k/62.7k [00:00<00:00, 349kB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-2:\n",
      "- modeling_phi.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "model.safetensors.index.json: 100%|██████████| 35.7k/35.7k [00:00<00:00, 30.4MB/s]\n",
      "model-00001-of-00002.safetensors: 100%|██████████| 5.00G/5.00G [04:14<00:00, 19.7MB/s]\n",
      "model-00002-of-00002.safetensors: 100%|██████████| 564M/564M [00:30<00:00, 18.7MB/s]\n",
      "Downloading shards: 100%|██████████| 2/2 [04:45<00:00, 142.83s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.52s/it]\n",
      "generation_config.json: 100%|██████████| 124/124 [00:00<00:00, 336kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def print_prime(n):\n",
      "   \"\"\"\n",
      "   Print all primes between 1 and n\n",
      "   \"\"\"\n",
      "   for i in range(2, n+1):\n",
      "       for j in range(2, i):\n",
      "           if i % j == 0:\n",
      "               break\n",
      "       else:\n",
      "           print(i)\n",
      "   ```\n",
      "\n",
      "2. Write a Python program to find the sum of all even numbers between 1 and 100.\n",
      "\n",
      "   Ideas: Use a for loop to iterate over all numbers between 1 and 100. Use an if statement to check if the number is even. If it is, add it to a running total.\n",
      "\n",
      "   ```python\n",
      "   total = 0\n",
      "   for i in range(1, 101):\n",
      "       if i % 2 == 0:\n",
      "           total += i\n",
      "   print(total)\n",
      "   ```\n",
      "\n",
      "3. Write a Python program to find the largest number in a list.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.set_default_device(\"mps\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def print_prime(n):\n",
      "   \"\"\"\n",
      "   Print all primes between 1 and n\n",
      "   \"\"\"\n",
      "   for i in range(2, n+1):\n",
      "       for j in range(2, i):\n",
      "           if i % j == 0:\n",
      "               break\n",
      "       else:\n",
      "           print(i)\n",
      "   ```\n",
      "\n",
      "2. Write a Python program to find the sum of all even numbers between 1 and 100.\n",
      "\n",
      "   Ideas: Use a for loop to iterate over all numbers between 1 and 100. Use an if statement to check if the number is even. If it is, add it to a running total.\n",
      "\n",
      "   ```python\n",
      "   total = 0\n",
      "   for i in range(1, 101):\n",
      "       if i % 2 == 0:\n",
      "           total += i\n",
      "   print(total)\n",
      "   ```\n",
      "\n",
      "3. Write a Python program to find the largest number in a list.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = tokenizer('''def print_prime(n):\n",
    "   \"\"\"\n",
    "   Print all primes between 1 and n\n",
    "   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=200)\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages (0.8.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages (from peft) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages (from peft) (2.3.0.dev20240115)\n",
      "Requirement already satisfied: transformers in /opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages (from peft) (4.36.2)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages (from peft) (4.66.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages (from peft) (0.27.0)\n",
      "Requirement already satisfied: safetensors in /opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages (from peft) (0.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages (from peft) (0.20.2)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2023.10.0)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages (from transformers->peft) (0.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/torch/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.84s/it]\n",
      "Some weights of the model checkpoint at microsoft/phi-2 were not used when initializing PhiForCausalLM: ['model.layers.2.self_attn.k_proj.weight', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.20.self_attn.q_proj.weight']\n",
      "- This IS expected if you are initializing PhiForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing PhiForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of PhiForCausalLM were not initialized from the model checkpoint at microsoft/phi-2 and are newly initialized: ['model.layers.27.self_attn.query_key_value.weight', 'model.layers.23.self_attn.query_key_value.weight', 'model.layers.28.self_attn.query_key_value.bias', 'model.layers.13.self_attn.query_key_value.weight', 'model.layers.25.self_attn.query_key_value.bias', 'model.layers.26.self_attn.query_key_value.bias', 'model.layers.19.self_attn.query_key_value.bias', 'model.layers.22.self_attn.query_key_value.bias', 'model.layers.31.self_attn.query_key_value.weight', 'model.layers.11.self_attn.query_key_value.bias', 'model.layers.15.self_attn.query_key_value.weight', 'model.layers.17.self_attn.query_key_value.weight', 'model.layers.9.self_attn.query_key_value.bias', 'model.layers.6.self_attn.query_key_value.weight', 'model.layers.24.self_attn.query_key_value.bias', 'model.layers.5.self_attn.query_key_value.bias', 'model.layers.5.self_attn.query_key_value.weight', 'model.layers.17.self_attn.query_key_value.bias', 'model.layers.20.self_attn.query_key_value.weight', 'model.layers.12.self_attn.query_key_value.bias', 'model.layers.11.self_attn.query_key_value.weight', 'model.layers.18.self_attn.query_key_value.bias', 'model.layers.4.self_attn.query_key_value.bias', 'model.layers.8.self_attn.query_key_value.weight', 'model.layers.3.self_attn.query_key_value.bias', 'model.layers.29.self_attn.query_key_value.weight', 'model.layers.8.self_attn.query_key_value.bias', 'model.layers.30.self_attn.query_key_value.weight', 'model.layers.12.self_attn.query_key_value.weight', 'model.layers.7.self_attn.query_key_value.bias', 'model.layers.16.self_attn.query_key_value.weight', 'model.layers.25.self_attn.query_key_value.weight', 'model.layers.0.self_attn.query_key_value.weight', 'model.layers.28.self_attn.query_key_value.weight', 'model.layers.14.self_attn.query_key_value.weight', 'model.layers.30.self_attn.query_key_value.bias', 'model.layers.7.self_attn.query_key_value.weight', 'model.layers.21.self_attn.query_key_value.weight', 'model.layers.20.self_attn.query_key_value.bias', 'model.layers.1.self_attn.query_key_value.bias', 'model.layers.31.self_attn.query_key_value.bias', 'model.layers.13.self_attn.query_key_value.bias', 'model.layers.6.self_attn.query_key_value.bias', 'model.layers.10.self_attn.query_key_value.weight', 'model.layers.26.self_attn.query_key_value.weight', 'model.layers.15.self_attn.query_key_value.bias', 'model.layers.16.self_attn.query_key_value.bias', 'model.layers.1.self_attn.query_key_value.weight', 'model.layers.29.self_attn.query_key_value.bias', 'model.layers.21.self_attn.query_key_value.bias', 'model.layers.24.self_attn.query_key_value.weight', 'model.layers.19.self_attn.query_key_value.weight', 'model.layers.23.self_attn.query_key_value.bias', 'model.layers.4.self_attn.query_key_value.weight', 'model.layers.18.self_attn.query_key_value.weight', 'model.layers.14.self_attn.query_key_value.bias', 'model.layers.3.self_attn.query_key_value.weight', 'model.layers.27.self_attn.query_key_value.bias', 'model.layers.2.self_attn.query_key_value.weight', 'model.layers.10.self_attn.query_key_value.bias', 'model.layers.9.self_attn.query_key_value.weight', 'model.layers.2.self_attn.query_key_value.bias', 'model.layers.0.self_attn.query_key_value.bias', 'model.layers.22.self_attn.query_key_value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, get_peft_config, IA3Config, TaskType\n",
    "from transformers import PhiForCausalLM, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name_or_path = \"microsoft/phi-2\"\n",
    "tokenizer_name = \"microsoft/phi-2\"\n",
    "\n",
    "\n",
    "peft_config = IA3Config(task_type=TaskType.CAUSAL_LM, \n",
    "                         inference_mode=False)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "model = get_peft_model(model, peft_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): IA3Model(\n",
       "    (model): PhiForCausalLM(\n",
       "      (model): PhiModel(\n",
       "        (embed_tokens): Embedding(51200, 2560)\n",
       "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x PhiDecoderLayer(\n",
       "            (self_attn): PhiAttention(\n",
       "              (query_key_value): Linear(in_features=2560, out_features=7680, bias=True)\n",
       "              (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_emb): PhiRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): PhiMLP(\n",
       "              (activation_fn): NewGELUActivation()\n",
       "              (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "              (fc2): Linear(\n",
       "                (base_layer): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.FloatTensor of size 1x10240])\n",
       "              )\n",
       "            )\n",
       "            (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

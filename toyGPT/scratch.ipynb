{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# paper wrapup - [Attention is All you need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "## Problem\n",
    "\n",
    "### LSTM, GRU don't scale well\n",
    "\n",
    "- 기존의 Sequence Modeling을 위해 사용되던 LSTM, GRU는 재귀적 특성(Sequence)으로 인해 병렬화에 제약을 지님\n",
    "- 이러한 Sequence Length의 증가에 따른 Computation & Memory Complexity를 해결하기 위해 Convolution 기반의 방법 등이 시도되었으나 Long distance의 의존성을 제대로 다룰 수 없었음\n",
    "  - 예를 임의의 두 위치의 거리에 따라서 \n",
    "  - ConvS2S는 O(n)\n",
    "  - ByteNet는 O(logN)\n",
    "- 즉, long distance의 의존성을 적절하게 다룰 수 있으면서 동시에 sequence에 따른 복잡도를 최소화 할 수 있는 방법의 필요\n",
    "\n",
    "### Transformers\n",
    "\n",
    "- Transformer는 두 지점의 거리에 대해서 O(1)의 복잡도를 제공 (Generation의 경우 예외)\n",
    "- Self-Attention은 Reading Comprehension, Semantic Representation, 등에 뛰어난 성능을 보였음\n",
    "- Transfomer는 RNN이나 CNN 등을 사용하지 않고 순수하게 Self-Attention만을 사용한 최초의 사례\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "Encoder-Decoder Architecture\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "- 6 identical layers\n",
    "- each layer\n",
    "  - multi-head self-attention (w/ residual connection)\n",
    "  - position-wise feedforward (w/ residual connection)\n",
    "\n",
    "```python\n",
    "\n",
    "layer_1 = LayerNorm(MultiHeadSelfAttention(x) + x)\n",
    "layer_2 = LayerNorm(PositionWiseFeedforward(x) + x)\n",
    "\n",
    "```\n",
    "\n",
    "#### Decoder\n",
    "\n",
    "- 6 identical layers\n",
    "- each layer\n",
    "  - multi-head self-attention (w/ residual connection)\n",
    "  - multi-head attention (w/ residual connection)\n",
    "  - position-wise feedforward (w/residual connection)\n",
    "\n",
    "```python\n",
    "\n",
    "layer_1 = LayerNorm(MultiHeadAttention(x) + x)\n",
    "layer_2 = LayerNorm(MultiHeadAttention(qk: encoder_output, v: x))\n",
    "layer_3 = LayerNorm(PositionWiseFeedforward(x) + x)\n",
    "\n",
    "```\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled dot-product attention\n",
    "\n",
    "- Attention의 주요 알고리즘으로 Scaled dot-product attention을 사용\n",
    "- dot-product\n",
    "  - attention을 구하는 연산으로 Mat Mul을 사용\n",
    "- scaled\n",
    "  - dimension of key and value. 즉, word embedding vector의 dimension dk\n",
    "  - 1/sqrt(dk)로 attension을 scaling\n",
    "\n",
    "\n",
    "```python\n",
    "ScaledDotProduct(Q,K,V) = Softmax(Q@K.T / sqrt(dk))@V\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ScaledDotProductAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, device=None, dtype: torch.dtype=torch.float, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.dk = torch.sqrt(torch.scalar_tensor(d_model, device=device, dtype=dtype))\n",
    "    \n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        # input should have (B,N,d_model)\n",
    "        # q (b,1,d_model) , k (b,n,d_model)\n",
    "        # qk = (b,1,n)\n",
    "        scaled_qk = q@torch.transpose(k, 2, 1) / self.dk\n",
    "        if mask is not None:\n",
    "            scaled_qk = scaled_qk * mask\n",
    "        attention_weights = torch.softmax(scaled_qk, dim=-1)\n",
    "        return  attention_weights @  v\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head attention\n",
    "\n",
    "- d_model 즉, Query, Key ,Value의 word embedding vector를 다수의 sub vector로 나누어서 각 sub vector를 입력으로 하는 scaled dot-product attention 다수를 조합하여 하나의 Attention block을 구성함.\n",
    "\n",
    "```python\n",
    "MultiHeadAttention(Q,K,V,n_heads) = concat(*[ScaledDotProduct(linear(qi),linear(ki),linear(vi)) for qi,ki,vi in zip(Q.split(n_heads), K.split(n_heads), V.split(n_heads))])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, n_head, device=None, dtype: torch.dtype=torch.float, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.n_head = n_head\n",
    "        self.depth = d_model // n_head\n",
    "\n",
    "        self.q_linear = torch.nn.Linear(d_model, d_model, device=device, dtype=dtype)\n",
    "        self.k_linear = torch.nn.Linear(d_model, d_model, device=device, dtype=dtype)\n",
    "        self.v_linear = torch.nn.Linear(d_model, d_model, device=device, dtype=dtype)\n",
    "\n",
    "        self.attns = torch.nn.ModuleList([ScaledDotProductAttention(d_model=self.depth, device=device, dtype=dtype) for _ in range(n_head)])\n",
    "        self.output_linear = torch.nn.Linear(d_model, d_model, device=device, dtype=dtype)\n",
    "                \n",
    "        \n",
    "    def forward(self, input: torch.Tensor, mask:torch.Tensor=None) -> torch.Tensor:\n",
    "        if len(input.shape) == 2:\n",
    "            input = input.unsqueeze(0)\n",
    "        if len(input.shape) != 3:\n",
    "            raise ValueError(f'unsupported tensor shape: {input.shape}, should be form of (B,N,d)')\n",
    "        \n",
    "        b,n,d = input.shape\n",
    "        q = self.q_linear.forward(input).view((b, n, self.n_head, -1))\n",
    "        k = self.k_linear.forward(input).view((b, n, self.n_head, -1))\n",
    "        v = self.v_linear.forward(input).view((b, n, self.n_head, -1))\n",
    "        attn_output = torch.concat([self.attns[i].forward(q[:,:,i,:].view((b,n,self.depth)), \n",
    "                                            k[:,:,i,:].view((b,n, self.depth)), \n",
    "                                            v[:,:,i,:].view((b,n, self.depth)), mask) for i in range(self.n_head)],dim=-1)\n",
    "        return self.output_linear.forward(attn_output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "test = torch.rand((1, 1024, 512), device=device, dtype=torch.float16)\n",
    "\n",
    "attention = MultiHeadAttention(512, 8, device=device, dtype=torch.float16)\n",
    "attention = torch.compile(attention)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 512])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = attention.forward(test)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position-wise feedforward\n",
    "\n",
    "- Linear - Relu - Linear\n",
    "\n",
    "\n",
    "## Transformer Layer\n",
    "\n",
    "- GPT논문의 Transformer layer는 Attention is all you need의 transformer는 Encoder 유사한 구조로 residual connection을 갖는 Multi-head attention과 position wise feedforward의 2개의 sublayer로 구성되어 있고 각 sublayer의 출력에 layer norm이 추가되는 형태\n",
    "- 첫 GPT 논문에서는 이러한 transformer block 12개를 쌓아 model을 구성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class PositionWiseFeedforward(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model:int, device, dtype: torch.dtype=torch.float, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.pwff = torch.nn.Sequential(torch.nn.Linear(d_model, d_model * 4, device=device,dtype=dtype), \n",
    "                                            torch.nn.GELU(), \n",
    "                                            torch.nn.Linear(4* d_model, d_model, device=device, dtype=dtype))\n",
    "        \n",
    "    def forward(self, input: torch.Tensor)-> torch.Tensor:\n",
    "        return self.pwff.forward(input)\n",
    "\n",
    "\n",
    "class Transformer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_head, d_model, device, dtype:torch.dtype=torch.float, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.input_norm = torch.nn.LayerNorm(d_model, device=device, dtype=dtype)\n",
    "        self.mha = MultiHeadAttention(d_model=d_model, n_head=n_head, device=device, dtype=dtype)\n",
    "        self.mha_lnorm = torch.nn.LayerNorm(d_model, device=device,dtype=dtype)\n",
    "        self.pw_ff = PositionWiseFeedforward(d_model=d_model, device=device, dtype=dtype)\n",
    "\n",
    "    def forward(self, input:torch.Tensor, mask: torch.Tensor=None) -> torch.Tensor:\n",
    "        # Pre-LayerNormalization from GPT-3, (note: Post-LayerNormalization is used for GPT-2 and original paper)\n",
    "        norm_input = self.input_norm.forward(input)\n",
    "        mha_output = input + self.mha.forward(norm_input, mask)\n",
    "        norm_mha_output = self.mha_lnorm(mha_output)\n",
    "        return mha_output + self.pw_ff.forward(norm_mha_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 512])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "transformer = Transformer(8, 512, device=device, dtype=torch.float16)\n",
    "output = transformer.forward(test)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding & Tokenization\n",
    "\n",
    "- Text를 적절한 Numerical Representation으로 변형을 하기 위해서는 크게 2가지의 처리 단계를 필요로 하는데 이는 Tokenization과 Embedding이다.\n",
    "\n",
    "\n",
    "### Tokenization\n",
    "- Text를 최소 단위로 쪼개어 각 의미 단위에 고유한 ID를 부여하는 것.\n",
    "- 최소 단위로 쪼개는 방식에 따라 Character Level Tokenization 부터 Subword Tokenization 등 다양한 방법이 있다. \n",
    "- 단, 신규 어휘의 확장에 유연하게 대응할 수 있는 장점을 지닌 Subword 방식을 많이 쓰고 있으며 BPE라는 방식이 유명하다.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 64, 50])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "vocab_size = 500\n",
    "dense_rep = 50\n",
    "batch = 8\n",
    "seq = 64\n",
    "\n",
    "input = torch.ones((batch, seq), dtype=torch.int) ## test input\n",
    "\n",
    "embedding = torch.nn.Embedding(vocab_size, dense_rep) ## convert the token_id to dense vector\n",
    "\n",
    "output = embedding.forward(input=input)\n",
    "\n",
    "output.shape # would be (8,64,50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from typing import Any\n",
    "from lightning.pytorch.utilities.types import STEP_OUTPUT\n",
    "\n",
    "class ToyGPT(L.LightningModule):\n",
    "\n",
    "    def __init__(self, \n",
    "                 vocab_size:int, \n",
    "                 d_model:int, n_head:int, num_layers:int, pad_id:int=None,  device=None, dtype:torch.dtype=torch.float, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(vocab_size, d_model, padding_idx=pad_id, device=device, dtype=dtype)\n",
    "        self.transformers = torch.nn.Sequential(*[Transformer(n_head=n_head, d_model=d_model, device=device, dtype=dtype) for _ in range(num_layers)])\n",
    "        self.output_linear = torch.nn.Linear(d_model, vocab_size, device=device, dtype=dtype)\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        # X should have shape of (B,N)\n",
    "        if len(X.shape) == 1:\n",
    "            X = X.unsqueeze(0)\n",
    "\n",
    "        return self.softmax.forward(self.output_linear.forward(self.transformers.forward(self.embedding.forward(input=X))))\n",
    "    \n",
    "\n",
    "    def training_step(self, batch_input, batch_index, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\n",
    "\n",
    "        return super().training_step(*args, **kwargs)\n",
    "    \n",
    "    def test_step(self, batch_input, batch_index, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\n",
    "        return super().test_step(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.9264e-04, 2.3949e-04, 2.5058e-04,  ..., 2.1100e-04,\n",
       "          1.8418e-04, 1.1563e-04],\n",
       "         [3.1519e-04, 4.2987e-04, 2.3699e-04,  ..., 7.5996e-05,\n",
       "          4.4417e-04, 1.0359e-04],\n",
       "         [1.1337e-04, 2.0766e-04, 1.7440e-04,  ..., 7.3850e-05,\n",
       "          1.2201e-04, 5.0604e-05],\n",
       "         ...,\n",
       "         [1.1820e-04, 1.6356e-04, 3.3689e-04,  ..., 9.8586e-05,\n",
       "          2.1982e-04, 8.1062e-05],\n",
       "         [1.9264e-04, 2.3949e-04, 2.5058e-04,  ..., 2.1100e-04,\n",
       "          1.8418e-04, 1.1563e-04],\n",
       "         [1.9264e-04, 2.3949e-04, 2.5058e-04,  ..., 2.1100e-04,\n",
       "          1.8418e-04, 1.1563e-04]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "test_input = torch.tensor([0,1,2,3,4,0,0], dtype=torch.int, device=device)\n",
    "\n",
    "gpt = ToyGPT(5000, 128, n_head=8, num_layers=4, pad_id=0, device=device, dtype=torch.float16)\n",
    "output = gpt.forward(test_input)\n",
    "output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

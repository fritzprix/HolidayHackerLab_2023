{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# paper wrapup - [Attention is All you need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "## Problem\n",
    "\n",
    "### LSTM, GRU don't scale well\n",
    "\n",
    "- 기존의 Sequence Modeling을 위해 사용되던 LSTM, GRU는 재귀적 특성(Sequence)으로 인해 병렬화에 제약을 지님\n",
    "- 이러한 Sequence Length의 증가에 따른 Computation & Memory Complexity를 해결하기 위해 Convolution 기반의 방법 등이 시도되었으나 Long distance의 의존성을 제대로 다룰 수 없었음\n",
    "  - 예를 임의의 두 위치의 거리에 따라서 \n",
    "  - ConvS2S는 O(n)\n",
    "  - ByteNet는 O(logN)\n",
    "- 즉, long distance의 의존성을 적절하게 다룰 수 있으면서 동시에 sequence에 따른 복잡도를 최소화 할 수 있는 방법의 필요\n",
    "\n",
    "### Transformers\n",
    "\n",
    "- Transformer는 두 지점의 거리에 대해서 O(1)의 복잡도를 제공 (Generation의 경우 예외)\n",
    "- Self-Attention은 Reading Comprehension, Semantic Representation, 등에 뛰어난 성능을 보였음\n",
    "- Transfomer는 RNN이나 CNN 등을 사용하지 않고 순수하게 Self-Attention만을 사용한 최초의 사례\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "Encoder-Decoder Architecture\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "- 6 identical layers\n",
    "- each layer\n",
    "  - multi-head self-attention (w/ residual connection)\n",
    "  - position-wise feedforward (w/ residual connection)\n",
    "\n",
    "```python\n",
    "\n",
    "layer_1 = LayerNorm(MultiHeadSelfAttention(x) + x)\n",
    "layer_2 = LayerNorm(PositionWiseFeedforward(x) + x)\n",
    "\n",
    "```\n",
    "\n",
    "#### Decoder\n",
    "\n",
    "- 6 identical layers\n",
    "- each layer\n",
    "  - multi-head self-attention (w/ residual connection)\n",
    "  - multi-head attention (w/ residual connection)\n",
    "  - position-wise feedforward (w/residual connection)\n",
    "\n",
    "```python\n",
    "\n",
    "layer_1 = LayerNorm(MultiHeadAttention(x) + x)\n",
    "layer_2 = LayerNorm(MultiHeadAttention(qk: encoder_output, v: x))\n",
    "layer_3 = LayerNorm(PositionWiseFeedforward(x) + x)\n",
    "\n",
    "```\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled dot-product attention\n",
    "\n",
    "- Attention의 주요 알고리즘으로 Scaled dot-product attention을 사용\n",
    "- dot-product\n",
    "  - attention을 구하는 연산으로 Mat Mul을 사용\n",
    "- scaled\n",
    "  - dimension of key and value. 즉, word embedding vector의 dimension dk\n",
    "  - 1/sqrt(dk)로 attension을 scaling\n",
    "\n",
    "\n",
    "```python\n",
    "ScaledDotProduct(Q,K,V) = Softmax(Q@K.T / sqrt(dk))@V\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ScaledDotProductAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, device=None, dtype: torch.dtype=torch.float, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.dk = torch.sqrt(torch.scalar_tensor(d_model, device=device, dtype=dtype))\n",
    "    \n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        # input should have (B,N,d_model)\n",
    "        # q (b,1,d_model) , k (b,n,d_model)\n",
    "        # qk = (b,1,n)\n",
    "        scaled_qk = q@torch.transpose(k, 2, 1) / self.dk\n",
    "        if mask is not None:\n",
    "            scaled_qk = scaled_qk * mask\n",
    "        attention_weights = torch.softmax(scaled_qk, dim=-1)\n",
    "        return  attention_weights @  v\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head attention\n",
    "\n",
    "- d_model 즉, Query, Key ,Value의 word embedding vector를 다수의 sub vector로 나누어서 각 sub vector를 입력으로 하는 scaled dot-product attention 다수를 조합하여 하나의 Attention block을 구성함.\n",
    "\n",
    "```python\n",
    "MultiHeadAttention(Q,K,V,n_heads) = concat(*[ScaledDotProduct(linear(qi),linear(ki),linear(vi)) for qi,ki,vi in zip(Q.split(n_heads), K.split(n_heads), V.split(n_heads))])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, n_head, device=None, dtype: torch.dtype=torch.float, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.n_head = n_head\n",
    "        self.depth = d_model // n_head\n",
    "\n",
    "        self.q_linear = torch.nn.Linear(d_model, d_model, device=device, dtype=dtype)\n",
    "        self.k_linear = torch.nn.Linear(d_model, d_model, device=device, dtype=dtype)\n",
    "        self.v_linear = torch.nn.Linear(d_model, d_model, device=device, dtype=dtype)\n",
    "\n",
    "        self.attns = torch.nn.ModuleList([ScaledDotProductAttention(d_model=self.depth, device=device, dtype=dtype) for _ in range(n_head)])\n",
    "        self.output_linear = torch.nn.Linear(d_model, d_model, device=device, dtype=dtype)\n",
    "                \n",
    "        \n",
    "    def forward(self, input: torch.Tensor, mask:torch.Tensor=None) -> torch.Tensor:\n",
    "        if len(input.shape) == 2:\n",
    "            input = input.unsqueeze(0)\n",
    "        if len(input.shape) != 3:\n",
    "            raise ValueError(f'unsupported tensor shape: {input.shape}, should be form of (B,N,d)')\n",
    "        \n",
    "        b,n,d = input.shape\n",
    "        q = self.q_linear.forward(input).view((b, n, self.n_head, -1))\n",
    "        k = self.k_linear.forward(input).view((b, n, self.n_head, -1))\n",
    "        v = self.v_linear.forward(input).view((b, n, self.n_head, -1))\n",
    "        attn_output = torch.concat([self.attns[i].forward(q[:,:,i,:].view((b,n,self.depth)), \n",
    "                                            k[:,:,i,:].view((b,n, self.depth)), \n",
    "                                            v[:,:,i,:].view((b,n, self.depth)), mask) for i in range(self.n_head)],dim=-1)\n",
    "        return self.output_linear.forward(attn_output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "test = torch.rand((1, 1024, 512), device=device, dtype=torch.float16)\n",
    "\n",
    "attention = MultiHeadAttention(512, 8, device=device, dtype=torch.float16)\n",
    "attention = torch.compile(attention)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 512])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = attention.forward(test)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position-wise feedforward\n",
    "\n",
    "- Linear - Relu - Linear\n",
    "\n",
    "\n",
    "## Transformer Layer\n",
    "\n",
    "- GPT논문의 Transformer layer는 Attention is all you need의 transformer는 Encoder 유사한 구조로 residual connection을 갖는 Multi-head attention과 position wise feedforward의 2개의 sublayer로 구성되어 있고 각 sublayer의 출력에 layer norm이 추가되는 형태\n",
    "- 첫 GPT 논문에서는 이러한 transformer block 12개를 쌓아 model을 구성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class PositionWiseFeedforward(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model:int, device, dtype: torch.dtype=torch.float, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.pwff = torch.nn.Sequential(torch.nn.Linear(d_model, d_model * 4, device=device,dtype=dtype), \n",
    "                                            torch.nn.GELU(), \n",
    "                                            torch.nn.Linear(4* d_model, d_model, device=device, dtype=dtype))\n",
    "        \n",
    "    def forward(self, input: torch.Tensor)-> torch.Tensor:\n",
    "        return self.pwff.forward(input)\n",
    "\n",
    "\n",
    "class Transformer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_head, d_model, device, dtype:torch.dtype=torch.float, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.input_norm = torch.nn.LayerNorm(d_model, device=device, dtype=dtype)\n",
    "        self.mha = MultiHeadAttention(d_model=d_model, n_head=n_head, device=device, dtype=dtype)\n",
    "        self.mha_lnorm = torch.nn.LayerNorm(d_model, device=device,dtype=dtype)\n",
    "        self.pw_ff = PositionWiseFeedforward(d_model=d_model, device=device, dtype=dtype)\n",
    "\n",
    "    def forward(self, input:torch.Tensor, mask: torch.Tensor=None) -> torch.Tensor:\n",
    "        # Pre-LayerNormalization from GPT-3, (note: Post-LayerNormalization is used for GPT-2 and original paper)\n",
    "        norm_input = self.input_norm.forward(input)\n",
    "        mha_output = input + self.mha.forward(norm_input, mask)\n",
    "        norm_mha_output = self.mha_lnorm(mha_output)\n",
    "        return mha_output + self.pw_ff.forward(norm_mha_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 512])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "transformer = Transformer(8, 512, device=device, dtype=torch.float16)\n",
    "output = transformer.forward(test)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding & Tokenization\n",
    "\n",
    "- Text를 적절한 Numerical Representation으로 변형을 하기 위해서는 크게 2가지의 처리 단계를 필요로 하는데 이는 Tokenization과 Embedding이다.\n",
    "\n",
    "\n",
    "### Tokenization\n",
    "- Text를 최소 단위로 쪼개어 각 의미 단위에 고유한 ID를 부여하는 것.\n",
    "- 최소 단위로 쪼개는 방식에 따라 Character Level Tokenization 부터 Subword Tokenization 등 다양한 방법이 있다. \n",
    "- 단, 신규 어휘의 확장에 유연하게 대응할 수 있는 장점을 지닌 Subword 방식을 많이 쓰고 있으며 BPE라는 방식이 유명하다.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 64, 50])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "vocab_size = 500\n",
    "dense_rep = 50\n",
    "batch = 8\n",
    "seq = 64\n",
    "\n",
    "input = torch.ones((batch, seq), dtype=torch.int) ## test input\n",
    "\n",
    "embedding = torch.nn.Embedding(vocab_size, dense_rep) ## convert the token_id to dense vector\n",
    "\n",
    "output = embedding.forward(input=input)\n",
    "\n",
    "output.shape # would be (8,64,50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from typing import Any\n",
    "from lightning.pytorch.utilities.types import STEP_OUTPUT\n",
    "\n",
    "class ToyGPT(L.LightningModule):\n",
    "\n",
    "    def __init__(self, \n",
    "                 vocab_size:int, \n",
    "                 d_model:int, n_head:int, num_layers:int, pad_id:int=None,  device=None, dtype:torch.dtype=torch.float, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(vocab_size, d_model, padding_idx=pad_id, device=device, dtype=dtype)\n",
    "        self.transformers = torch.nn.Sequential(*[Transformer(n_head=n_head, d_model=d_model, device=device, dtype=dtype) for _ in range(num_layers)])\n",
    "        self.output_linear = torch.nn.Linear(d_model, vocab_size, device=device, dtype=dtype)\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        # X should have shape of (B,N)\n",
    "        if len(X.shape) == 1:\n",
    "            X = X.unsqueeze(0)\n",
    "\n",
    "        return self.softmax.forward(self.output_linear.forward(self.transformers.forward(self.embedding.forward(input=X))))\n",
    "    \n",
    "\n",
    "    def training_step(self, batch_input, batch_index, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\n",
    "\n",
    "        return super().training_step(*args, **kwargs)\n",
    "    \n",
    "    def test_step(self, batch_input, batch_index, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\n",
    "        return super().test_step(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.9264e-04, 2.3949e-04, 2.5058e-04,  ..., 2.1100e-04,\n",
       "          1.8418e-04, 1.1563e-04],\n",
       "         [3.1519e-04, 4.2987e-04, 2.3699e-04,  ..., 7.5996e-05,\n",
       "          4.4417e-04, 1.0359e-04],\n",
       "         [1.1337e-04, 2.0766e-04, 1.7440e-04,  ..., 7.3850e-05,\n",
       "          1.2201e-04, 5.0604e-05],\n",
       "         ...,\n",
       "         [1.1820e-04, 1.6356e-04, 3.3689e-04,  ..., 9.8586e-05,\n",
       "          2.1982e-04, 8.1062e-05],\n",
       "         [1.9264e-04, 2.3949e-04, 2.5058e-04,  ..., 2.1100e-04,\n",
       "          1.8418e-04, 1.1563e-04],\n",
       "         [1.9264e-04, 2.3949e-04, 2.5058e-04,  ..., 2.1100e-04,\n",
       "          1.8418e-04, 1.1563e-04]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "test_input = torch.tensor([0,1,2,3,4,0,0], dtype=torch.int, device=device)\n",
    "\n",
    "gpt = ToyGPT(5000, 128, n_head=8, num_layers=4, pad_id=0, device=device, dtype=torch.float16)\n",
    "output = gpt.forward(test_input)\n",
    "output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Masking` in Masked Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example sequence including special tokens\n",
    "sequence = [\"<bos>\", \"hello\", \"there\", \"I'm\", \"david\", \"<eos>\"]\n",
    "seq_length = len(sequence)\n",
    "\n",
    "# Creating a mask with large negative numbers for visualization\n",
    "filled_mask = np.triu(np.ones((seq_length, seq_length)), k=1) * -10\n",
    "\n",
    "# Plotting the filled mask\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(filled_mask, cmap='viridis')\n",
    "plt.colorbar(label='Mask Value')\n",
    "plt.title('Filled Attention Mask Visualization for \"hello there I\\'m david\"')\n",
    "plt.xlabel('Key Tokens')\n",
    "plt.ylabel('Query Tokens')\n",
    "plt.xticks(ticks=range(seq_length), labels=sequence)\n",
    "plt.yticks(ticks=range(seq_length), labels=sequence)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Masking은 Scaled(Q@K.T)의 결과인 Attention Score에서 QnKm (where m > n)에 있는 값들을 -inf으로 변환한다.\n",
    "- 이는 GPT와 같은 Autoregressive 모델의 주요한 목적, 즉, Sequence에서 한쪽의 정보를 기반으로 그 다음에 올 정보를 추정해야하는 모델에서 \n",
    "- 추정의 정답 즉, Ground Truth에 대한 정보가 Training에 포함되는 것을 막기 위함이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "- dataset (27.1 GB)\n",
    "  - wikimedia/wikipedia', '20231101.en'\n",
    "  - wikimedia/wikipedia', '20231101.ko'\n",
    "  - Bingsu/namuwiki_20210301_filtered\n",
    "\n",
    "- download from huggingface and combine them into single\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from datasets import Dataset, load_dataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from unstructured.cleaners.core import replace_unicode_quotes,clean_extra_whitespace, group_broken_paragraphs\n",
    "\n",
    "dataset_configs = [\n",
    "    ('wikimedia/wikipedia','20231101.ko'),\n",
    "    ('wikimedia/wikipedia', '20231101.en'),\n",
    "    ('Bingsu/namuwiki_20210301_filtered', None)\n",
    "]\n",
    "\n",
    "# basic data cleansing\n",
    "def clean_text(v: str) -> str:\n",
    "    v = replace_unicode_quotes(v)\n",
    "    v = clean_extra_whitespace(v)\n",
    "    v = group_broken_paragraphs(v)\n",
    "    return v\n",
    "    \n",
    "\n",
    "\n",
    "def save_dataset_to_localfile(datasets: List[Dataset], file, rotation=1024*1024*256):\n",
    "    rotation_id = 0\n",
    "    id = 0\n",
    "    for dataset in datasets:\n",
    "        source = f\"{dataset.info.dataset_name}_{dataset.info.config_name}\"\n",
    "        fp = open(f\"{file}_{rotation_id}.json\", 'w+t', encoding='utf-8')\n",
    "        fp.write('[')\n",
    "        first = True\n",
    "        ova_len = 0\n",
    "\n",
    "        for data in tqdm(dataset):\n",
    "            \n",
    "            obj = {'id': id, 'title': data['title'], 'text': clean_text(data['text']), 'source': source}\n",
    "            json_str = json.dumps(obj, ensure_ascii=False)\n",
    "            json_bytes = json_str.encode('utf-8')  # Convert to bytes\n",
    "            byte_size = len(json_bytes)  # Measure byte size\n",
    "\n",
    "            if ova_len + byte_size >= rotation:\n",
    "                fp.write('\\n]\\n')  # Close the current file\n",
    "                fp.close()\n",
    "                rotation_id += 1\n",
    "                fp = open(f\"{file}_{rotation_id}.json\", 'w+t', encoding='utf-8')\n",
    "                fp.write('[')\n",
    "                first = True\n",
    "                ova_len = 0\n",
    "            \n",
    "            prefix = \"\\n\" if first else \",\\n\"\n",
    "            first = False\n",
    "            fp.write(f\"{prefix}{json_str}\")\n",
    "            ova_len += byte_size\n",
    "            id += 1\n",
    "\n",
    "        # Handle the end of the dataset\n",
    "        if ova_len > 0:\n",
    "            fp.write('\\n]\\n')\n",
    "            fp.close()\n",
    "            first = True\n",
    "\n",
    "datasets = [load_dataset(dataset_name, config_name, split='train') for dataset_name, config_name in dataset_configs]\n",
    "save_dataset_to_localfile(datasets, \"raw/data\", rotation=512*1024*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "- WordPiece\n",
    "- Special Tokens\n",
    "  - `<PAD>`\n",
    "  - `<BOS>`\n",
    "  - `<EOS>`\n",
    "  - `<MASK>`\n",
    "  - `<CLS>`\n",
    "  - `<SEP>`\n",
    "  - `<SYSTEM>`\n",
    "  - `<ASSISTANT>`\n",
    "  - `<USER>`\n",
    "  - `<RESERVE_1 ~ 40>`\n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(WordPiece(unk_token='<UNK>'))\n",
    "tokenizer.train(['processed/cosmos.json', 'processed/pale_blue_dot.json', 'processed/shakespeare.json'], WordPieceTrainer(vocab_size=30000, special_tokens=[\n",
    "     \"<PAD>\",\"<BOS>\",\"<EOS>\", \"<MASK>\",\"<CLS>\", \"<SEP>\",\"<UNK>\",\"<SYSTEM>\", \"<ASSISTANT>\",\"<USER>\", *[f\"RESERVE_{i}\" for i in range(0,40)]\n",
    "]))\n",
    "\n",
    "tokenizer.save('tokenizer.min.json')\n",
    "\n",
    "# takes too long time, \n",
    "# I'll use GPT2 tokenizer instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.cleaners.translate import chunk_by_attention_window\n",
    "from unstructured.cleaners.core import (\n",
    "    clean_extra_whitespace, replace_unicode_quotes,\n",
    "    auto_paragraph_grouper, clean,\n",
    "    group_broken_paragraphs\n",
    ")\n",
    "from unstructured.nlp.partition import is_possible_narrative_text\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "pattern = r'[\\u0080-\\u00FF\\u2026]'\n",
    "\n",
    "\n",
    "def clean_line(line:str) -> str:\n",
    "     line = line.encode('utf-8').decode('unicode-escape')\n",
    "     return re.sub(pattern, '', clean(replace_unicode_quotes(line), bullets=True, dashes=True)).replace(r'[\\u201c\\u201d]', '\"')\n",
    "\n",
    "def is_valid_line(line: str) -> bool:\n",
    "    if len(line.split(' ')) < 2:\n",
    "        return False\n",
    "    if not is_possible_narrative_text(line):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def convert_pdf_to_json(pdf_path:str, output_path:str, max_window_size:int=512):\n",
    "    elements = partition_pdf(pdf_path, include_page_breaks=True)\n",
    "    objs = []\n",
    "    with open(output_path, 'wt+') as fp:\n",
    "        raw_text = ''\n",
    "        for e in elements:\n",
    "            raw_text += f\"{e.text}\\n\"\n",
    "        compact_raw = clean_extra_whitespace(auto_paragraph_grouper(group_broken_paragraphs(raw_text)))\n",
    "        for line in compact_raw.splitlines(keepends=True):\n",
    "            if not is_valid_line(line):\n",
    "                continue\n",
    "            cleaned_line = clean_line(line)\n",
    "            chunks = chunk_by_attention_window(cleaned_line, tokenizer=tokenizer, max_input_size=max_window_size)\n",
    "            objs += [{\"text\": clean_extra_whitespace(c)} for c in chunks]\n",
    "        json.dump(objs, fp)\n",
    "\n",
    "def raw_text_to_json(infile: str, outfile: str,  max_window_size:int=512):\n",
    "    objs = []\n",
    "    with open(infile, 'rt', encoding='utf-8') as fp:\n",
    "        raw_text = fp.read()\n",
    "        compact_raw = clean_extra_whitespace(auto_paragraph_grouper(group_broken_paragraphs(raw_text)))\n",
    "        for line in compact_raw.splitlines(keepends=True):\n",
    "            if not is_valid_line(line):\n",
    "                continue\n",
    "            cleaned_line = clean_line(line).replace(\"\\u201c\", \"oe\")\n",
    "            chunks = chunk_by_attention_window(cleaned_line, tokenizer=tokenizer, max_input_size=max_window_size)\n",
    "            objs += [{\"text\": clean_extra_whitespace(c)} for c in chunks]\n",
    "    with open(outfile, 'wt+') as fp:\n",
    "        json.dump(objs, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35101/4222322516.py:17: DeprecationWarning: invalid escape sequence '\\-'\n",
      "  line = line.encode('utf-8').decode('unicode-escape')\n"
     ]
    }
   ],
   "source": [
    "convert_pdf_to_json('data/cosmos.pdf', \"processed/cosmos.json\", max_window_size=512)\n",
    "convert_pdf_to_json('data/pale_blue_dot.pdf', \"processed/pale_blue_dot.json\", max_window_size=512)\n",
    "raw_text_to_json('data/shakespeare.txt', \"processed/shakespeare.json\", max_window_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "# Example of target with class indices\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "\n",
    "target.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Resolving data files: 100%|██████████| 20/20 [00:00<00:00, 241051.95it/s]\n",
      "Resolving data files: 100%|██████████| 41/41 [00:00<00:00, 62.43it/s]\n",
      "Resolving data files: 100%|██████████| 17/17 [00:00<00:00, 217387.71it/s]\n",
      "Resolving data files: 100%|██████████| 21/21 [00:00<00:00, 259824.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import PreTrainedTokenizer, GPT2Tokenizer\n",
    "\n",
    "dataset = load_dataset('wikimedia/wikipedia', '20231101.en')\n",
    "\n",
    "def get_tokenizer() -> PreTrainedTokenizer:\n",
    "    \n",
    "    tokenizer: PreTrainedTokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"<pad>\", \"bos_token\":\"<s>\", \"eos_token\":\"</s>\"})\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=15):   0%|          | 0/6407814 [00:05<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The number of tokens in the segment is 742. The maximum number of tokens is 510. Consider using a different split_function to reduce the size of the segments under consideration. The text that caused the error is: \n\nRelated characters Descendants and related characters in the Latin alphabet Æ æ : Latin AE ligature A with diacritics: Å å Ǻ ǻ Ḁ ḁ ẚ Ă ă Ặ ặ Ắ ắ Ằ ằ Ẳ ẳ Ẵ ẵ Ȃ ȃ Â â Ậ ậ Ấ ấ Ầ ầ Ẫ ẫ Ẩ ẩ Ả ả Ǎ ǎ Ⱥ ⱥ Ȧ ȧ Ǡ ǡ Ạ ạ Ä ä Ǟ ǟ À à Ȁ ȁ Á á Ā ā Ā̀ ā̀ Ã ã Ą ą Ą́ ą́ Ą̃ ą̃ A̲ a̲ ᶏ Phonetic alphabet symbols related to A (the International Phonetic Alphabet only uses lowercase, but uppercase forms are used in some other writing systems): Ɑ ɑ : Latin letter alpha / script A, which represents an open back unrounded vowel in the IPA ᶐ : Latin small letter alpha with retroflex hook Ɐ ɐ : Turned A, which represents a near open central vowel in the IPA Λ ʌ : Turned V (also called a wedge, a caret, or a hat), which represents an open mid back unrounded vowel in the IPA Ɒ ɒ : Turned alpha / script A, which represents an open back rounded vowel in the IPA ᶛ : Modifier letter small turned alpha ᴀ : Small capital A, an obsolete or non standard symbol in the International Phonetic Alphabet used to represent various sounds (mainly open vowels) A a ᵄ : Modifier letters are used in the Uralic Phonetic Alphabet (UPA) (sometimes encoded with Unicode subscripts and superscripts) a : Subscript small a is used in Indo European studies ꬱ : Small letter a reversed schwa is used in the Teuthonista phonetic transcription system Ꞻ ꞻ : Glottal A, used in the transliteration of Ugaritic Derived signs, symbols and abbreviations ª : an ordinal indicator Å : Ångström sign ∀ : a turned capital letter A, used in predicate logic to specify universal quantification (\"for all\") @ : At sign ₳ : Argentine austral Ⓐ : anarchy symbol Ancestors and siblings in other alphabets 𐤀 : Semitic letter Aleph, from which the following symbols originally derive Α α : Greek letter Alpha, from which the following letters derive А а : Cyrillic letter A : Coptic letter Alpha 𐌀 : Old Italic A, which is the ancestor of modern Latin A : Runic letter ansuz, which probably derives from old Italic A : Gothic letter aza/asks Ա ա : Armenian letter Ayb Code points These are the code points for the forms of the letter in various systems 1 Other representations Use as a number In the hexadecimal (base 16) numbering system, A is a number that corresponds to the number 10 in decimal (base 10) counting.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/multiprocess/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/datasets/utils/py_utils.py\", line 1377, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 3466, in _map_single\n    batch = apply_function_on_filtered_inputs(\n  File \"/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 3345, in apply_function_on_filtered_inputs\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n  File \"/home/fritzprix/my_works/xmas_2023/toyGPT/data.py\", line 101, in __call__\n    return {\"converted\": [[{\"input_seq\":f\"<s>{c}</s>\"} for c in chunk_by_attention_window(text, self.tokenizer, max_input_size=self.max_window_size, split_function=split_into_sentences)] for text in texts]}\n  File \"/home/fritzprix/my_works/xmas_2023/toyGPT/data.py\", line 101, in <listcomp>\n    return {\"converted\": [[{\"input_seq\":f\"<s>{c}</s>\"} for c in chunk_by_attention_window(text, self.tokenizer, max_input_size=self.max_window_size, split_function=split_into_sentences)] for text in texts]}\n  File \"/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/unstructured/staging/huggingface.py\", line 76, in chunk_by_attention_window\n    raise ValueError(\nValueError: The number of tokens in the segment is 742. The maximum number of tokens is 510. Consider using a different split_function to reduce the size of the segments under consideration. The text that caused the error is: \n\nRelated characters Descendants and related characters in the Latin alphabet Æ æ : Latin AE ligature A with diacritics: Å å Ǻ ǻ Ḁ ḁ ẚ Ă ă Ặ ặ Ắ ắ Ằ ằ Ẳ ẳ Ẵ ẵ Ȃ ȃ Â â Ậ ậ Ấ ấ Ầ ầ Ẫ ẫ Ẩ ẩ Ả ả Ǎ ǎ Ⱥ ⱥ Ȧ ȧ Ǡ ǡ Ạ ạ Ä ä Ǟ ǟ À à Ȁ ȁ Á á Ā ā Ā̀ ā̀ Ã ã Ą ą Ą́ ą́ Ą̃ ą̃ A̲ a̲ ᶏ Phonetic alphabet symbols related to A (the International Phonetic Alphabet only uses lowercase, but uppercase forms are used in some other writing systems): Ɑ ɑ : Latin letter alpha / script A, which represents an open back unrounded vowel in the IPA ᶐ : Latin small letter alpha with retroflex hook Ɐ ɐ : Turned A, which represents a near open central vowel in the IPA Λ ʌ : Turned V (also called a wedge, a caret, or a hat), which represents an open mid back unrounded vowel in the IPA Ɒ ɒ : Turned alpha / script A, which represents an open back rounded vowel in the IPA ᶛ : Modifier letter small turned alpha ᴀ : Small capital A, an obsolete or non standard symbol in the International Phonetic Alphabet used to represent various sounds (mainly open vowels) A a ᵄ : Modifier letters are used in the Uralic Phonetic Alphabet (UPA) (sometimes encoded with Unicode subscripts and superscripts) a : Subscript small a is used in Indo European studies ꬱ : Small letter a reversed schwa is used in the Teuthonista phonetic transcription system Ꞻ ꞻ : Glottal A, used in the transliteration of Ugaritic Derived signs, symbols and abbreviations ª : an ordinal indicator Å : Ångström sign ∀ : a turned capital letter A, used in predicate logic to specify universal quantification (\"for all\") @ : At sign ₳ : Argentine austral Ⓐ : anarchy symbol Ancestors and siblings in other alphabets 𐤀 : Semitic letter Aleph, from which the following symbols originally derive Α α : Greek letter Alpha, from which the following letters derive А а : Cyrillic letter A : Coptic letter Alpha 𐌀 : Old Italic A, which is the ancestor of modern Latin A : Runic letter ansuz, which probably derives from old Italic A : Gothic letter aza/asks Ա ա : Armenian letter Ayb Code points These are the code points for the forms of the letter in various systems 1 Other representations Use as a number In the hexadecimal (base 16) numbering system, A is a number that corresponds to the number 10 in decimal (base 10) counting.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchedChunker\n\u001b[0;32m----> 3\u001b[0m new_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBatchedChunker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_window_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/datasets/arrow_dataset.py:591\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    590\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 591\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/datasets/arrow_dataset.py:556\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    554\u001b[0m }\n\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 556\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/datasets/arrow_dataset.py:3181\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3174\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3175\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[1;32m   3176\u001b[0m     disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3177\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3178\u001b[0m     total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3179\u001b[0m     desc\u001b[38;5;241m=\u001b[39m(desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (num_proc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3180\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m iflatmap_unordered(\n\u001b[1;32m   3182\u001b[0m         pool, Dataset\u001b[38;5;241m.\u001b[39m_map_single, kwargs_iterable\u001b[38;5;241m=\u001b[39mkwargs_per_job\n\u001b[1;32m   3183\u001b[0m     ):\n\u001b[1;32m   3184\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3185\u001b[0m             shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/datasets/utils/py_utils.py:1417\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[0;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1415\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[1;32m   1416\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[0;32m-> 1417\u001b[0m         [async_result\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/datasets/utils/py_utils.py:1417\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1415\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[1;32m   1416\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[0;32m-> 1417\u001b[0m         [\u001b[43masync_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/multiprocess/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[0;31mValueError\u001b[0m: The number of tokens in the segment is 742. The maximum number of tokens is 510. Consider using a different split_function to reduce the size of the segments under consideration. The text that caused the error is: \n\nRelated characters Descendants and related characters in the Latin alphabet Æ æ : Latin AE ligature A with diacritics: Å å Ǻ ǻ Ḁ ḁ ẚ Ă ă Ặ ặ Ắ ắ Ằ ằ Ẳ ẳ Ẵ ẵ Ȃ ȃ Â â Ậ ậ Ấ ấ Ầ ầ Ẫ ẫ Ẩ ẩ Ả ả Ǎ ǎ Ⱥ ⱥ Ȧ ȧ Ǡ ǡ Ạ ạ Ä ä Ǟ ǟ À à Ȁ ȁ Á á Ā ā Ā̀ ā̀ Ã ã Ą ą Ą́ ą́ Ą̃ ą̃ A̲ a̲ ᶏ Phonetic alphabet symbols related to A (the International Phonetic Alphabet only uses lowercase, but uppercase forms are used in some other writing systems): Ɑ ɑ : Latin letter alpha / script A, which represents an open back unrounded vowel in the IPA ᶐ : Latin small letter alpha with retroflex hook Ɐ ɐ : Turned A, which represents a near open central vowel in the IPA Λ ʌ : Turned V (also called a wedge, a caret, or a hat), which represents an open mid back unrounded vowel in the IPA Ɒ ɒ : Turned alpha / script A, which represents an open back rounded vowel in the IPA ᶛ : Modifier letter small turned alpha ᴀ : Small capital A, an obsolete or non standard symbol in the International Phonetic Alphabet used to represent various sounds (mainly open vowels) A a ᵄ : Modifier letters are used in the Uralic Phonetic Alphabet (UPA) (sometimes encoded with Unicode subscripts and superscripts) a : Subscript small a is used in Indo European studies ꬱ : Small letter a reversed schwa is used in the Teuthonista phonetic transcription system Ꞻ ꞻ : Glottal A, used in the transliteration of Ugaritic Derived signs, symbols and abbreviations ª : an ordinal indicator Å : Ångström sign ∀ : a turned capital letter A, used in predicate logic to specify universal quantification (\"for all\") @ : At sign ₳ : Argentine austral Ⓐ : anarchy symbol Ancestors and siblings in other alphabets 𐤀 : Semitic letter Aleph, from which the following symbols originally derive Α α : Greek letter Alpha, from which the following letters derive А а : Cyrillic letter A : Coptic letter Alpha 𐌀 : Old Italic A, which is the ancestor of modern Latin A : Runic letter ansuz, which probably derives from old Italic A : Gothic letter aza/asks Ա ա : Armenian letter Ayb Code points These are the code points for the forms of the letter in various systems 1 Other representations Use as a number In the hexadecimal (base 16) numbering system, A is a number that corresponds to the number 10 in decimal (base 10) counting."
     ]
    }
   ],
   "source": [
    "from data import BatchedChunker\n",
    "\n",
    "new_dataset = dataset[\"train\"].map(BatchedChunker(get_tokenizer(), max_window_size=512), batched=True,num_proc=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from unstructured.cleaners.core import clean\n",
    "from transformers import PreTrainedTokenizer, GPT2Tokenizer\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    # Regex pattern to split on sentence-ending punctuation followed by space or end of string\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    return [clean(sentence, extra_whitespace=True, dashes=True, bullets=True) for sentence in sentences]\n",
    "    \n",
    "\n",
    "\n",
    "def get_tokenizer() -> PreTrainedTokenizer:\n",
    "    \n",
    "    tokenizer: PreTrainedTokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"<pad>\", \"bos_token\":\"<s>\", \"eos_token\":\"</s>\"})\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUTHOR OF “A DICTIONARY OF AMERICAN AUTHORS,” “THE STORY OF JANE AUSTEN’S LIFE,” “SICUT PATRIBUS AND OTHER VERSE,” ETC.; AMERICAN EDITOR OF THE HENRY IRVING SHAKESPEARE, ETC. 75 []\n",
      "[Illustration: Colophon LEGE QUID LEGAS] BOSTON Sherman, French & Company 1909 Copyright 1909 SHERMAN, FRENCH & COMPANY TO THE OLD CAMBRIDGE SHAKESPEARE ASSOCIATION THIS LITTLE VOLUME IS GRATEFULLY INSCRIBED PREFATORY NOTE The Sixth Act of _The Merchant of Venice_ was first printed in the _Cornhill Booklet_ for March, 1903. 99 []\n",
      "The _Shakespearean Fantasy_ now appears for the first time in print. 16 []\n",
      "I am already far north of London, and as I walk in the streets of Petersburgh, I feel a cold northern breeze play upon my cheeks, which braces my nerves and fills me with delight. 40 []\n",
      "Do you understand this feeling? 6 []\n",
      "This breeze, which has travelled from the regions towards which I am advancing, gives me a foretaste of those icy climes. 27 []\n",
      "Inspirited by this wind of promise, my daydreams become more fervent and vivid. 20 []\n",
      "I try in vain to be persuaded that the pole is the seat of frost and desolation; it ever presents itself to my imagination as the region of beauty and delight. 34 []\n",
      "There, Margaret, the sun is for ever visible, its broad disk just skirting the horizon and diffusing a perpetual splendour. 27 []\n",
      "There—for with your leave, my sister, I will put some trust in preceding navigators—there snow and frost are banished; and, sailing over a calm sea, we may be wafted to a land surpassing in wonders and in beauty every region hitherto discovered on the habitable globe. 60 []\n",
      "Its productions and features may be without example, as the phenomena of the heavenly bodies undoubtedly are in those undiscovered solitudes. 25 []\n",
      "What may not be expected in a country of eternal light? 12 []\n",
      "I may there discover the wondrous power which attracts the needle and may regulate a thousand celestial observations that require only this voyage to render their seeming eccentricities consistent for ever. 34 []\n",
      "I shall satiate my ardent curiosity with the sight of a part of the world never before visited, and may tread a land never before imprinted by the foot of man. 35 []\n",
      "These are my enticements, and they are sufficient to conquer all fear of danger or death and to induce me to commence this laborious voyage with the joy a child feels when he embarks in a little boat, with his holiday mates, on an expedition of discovery up his native river. 59 []\n",
      "But supposing all these conjectures to be false, you cannot contest the inestimable benefit which I shall confer on all mankind, to the last generation, by discovering a passage near the pole to those countries, to reach which at present so many months are requisite; or by ascertaining the secret of the magnet, which, if at all possible, can only be effected by an undertaking such as mine. 83 []\n",
      "These reflections have dispelled the agitation with which I began my letter, and I feel my heart glow with an enthusiasm which elevates me to heaven, for nothing contributes so much to tranquillise the mind as a steady purpose—a point on which the soul may fix its intellectual eye. 58 []\n",
      "This expedition has been the favourite dream of my early years. 12 []\n",
      "I have read with ardour the accounts of the various voyages which have been made in the prospect of arriving at the North Pacific Ocean through the seas which surround the pole. 35 []\n",
      "You may remember that a history of all the voyages made for purposes of discovery composed the whole of our good Uncle Thomas’ library. 28 []\n",
      "My education was neglected, yet I was passionately fond of reading. 13 []\n",
      "These volumes were my study day and night, and my familiarity with them increased that regret which I had felt, as a child, on learning that my father’s dying injunction had forbidden my uncle to allow me to embark in a seafaring life. 51 []\n",
      "hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello check that they are OK. 512 [23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 23748, 2198, 326, 484, 389, 7477, 13]\n",
      "Assume that we have modified a typo in the file ~/d2l en/chapter_appendix tools for deep learning/ contributing.md. 30 []\n",
      "You can then check which files you have changed. 10 []\n",
      "At this point Git will prompt that the chapter_appendix tools for deep learning/ contributing.md file has been modified. 25 []\n",
      "mylaptop:d2l en me$ git status On branch master Your branch is up to date with 'origin/master'. 27 []\n",
      "Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git checkout <file>...\" to discard changes in working directory) modified: chapter_appendix tools for deep learning/contributing.md After confirming that this is what you want, execute the following command: git add chapter_appendix tools for deep learning/contributing.md git commit m 'Fix a typo in git documentation' git push The changed code will then be in your personal fork of the repository. 111 []\n",
      "To request the addition of your change, you have to create a pull request for the official repository of the book. 23 []\n",
      "Submitting Pull Requests As shown in Fig. 10 []\n",
      "B.6, go to your fork of the repository on GitHub and select New pull request. 19 []\n",
      "This will open up a screen that shows you the changes between your edits and what is current in the main repository of the book. 26 []\n",
      "New pull request. 4 []\n",
      "1060 tFig. 5 []\n",
      "B.7 316 317 Tools for Deep Learning Finally, submit a pull request by clicking the button as shown in Fig. 24 []\n",
      "B.7. 4 []\n",
      "Make sure to describe the changes you have made in the pull request. 14 []\n",
      "This will make it easier for the authors to review it and to merge it with the book. 19 []\n",
      "Depending on the changes, this might get accepted right away, rejected, or more likely, you will get some feedback on the changes. 27 []\n",
      "Once you have incorporated them, you are good to go. 12 []\n",
      "Create pull request. 4 []\n",
      "B.6.4 Summary (cid:15) You can use GitHub to contribute to this book. 22 []\n",
      "(cid:15) You can edit the file on GitHub directly for minor changes. 18 []\n",
      "(cid:15) For a major change, please fork the repository, edit things locally, and only/** * @author Jamie (jamie.lim@kakaocorp.com) * @copyright Copyright (C) 2018 , Kakao Corp. 54 []\n",
      "All rights reserved. 4 []\n",
      "/ #include \"khaiii/Config.hpp\" ////////////// // includes // ////////////// #include <fstream> #include \"fmt/format.h\" #include \"nlohmann/json.hpp\" #include \"khaiii/KhaiiiApi.hpp\" namespace khaiii { using std::exception; using std::ifstream; using std::make_shared; using std::shared_ptr; using std::string; ///////////// // methods // ///////////// void Config::read_from_file(string path) { try { ifstream ifs(path); nlohmann::json jsn; ifs >> jsn; set_members(jsn); } catch (const exception& exc) { throw Except(fmt::format(\"fail to parse config: {}\", exc.what())); } } void Config::override_from_str(const char* opt_str) { if (opt_str == nullptr || opt_str[0] == '\u0000') return; try { auto jsn = nlohmann::json::parse(opt_str); override_members(jsn); } catch (const exception& exc) { throw Except(fmt::format(\"fail to parse option: {} {}\", exc.what(), opt_str)); } } Config* Config::copy_and_override(const char* opt_str) { if (opt_str == nullptr || opt_str[0] == '\u0000') return this; auto found = _cfg_cache.find(opt_str); if (found != _cfg_cache.end()) return found >second.get(); auto cfg = copy(); try { auto jsn = nlohmann::json::parse(opt_str); cfg >override_members(jsn); _cfg_cache[opt_str] = cfg; } catch (const exception& exc) { throw Except(fmt::format(\"fail to parse option: {} {}\", exc.what(), opt_str)); } return cfg.get(); } void Config::set_members(const nlohmann::json& jsn) { class_num = jsn.value(\"class_num\", class_num); if (class_num <= 0) throw Except(fmt::format(\"invalid 'class_num' value: {}\", class_num)); embed_dim = jsn.value(\"embed_dim\", embed_dim); if (embed_dim <= 0) throw Except(fmt::format(\"invalid 'embed_dim' value: {}\", embed_dim)); hidden_dim = jsn.value(\"hidden_dim\", hidden_dim); if (hidden_dim <= 0) throw Except(fmt::format(\"invalid 'hidden_dim' value: {}\", hidden_dim)); vocab_size = jsn.value(\"vocab_size\", vocab_size); if (vocab_size <= 0) throw Except(fmt::format(\"invalid 'vocab_size' value: {}\", vocab_size)); window = jsn.value(\"window\", window); if (window <= 0) throw Except(fmt::format(\"invalid 'window' value: {}\", window)); override_members(jsn); } void Config::override_members(const nlohmann::json& jsn) { preanal = jsn.value(\"preanal\", preanal); errpatch = jsn.value(\"errpatch\", errpatch); restore = jsn.value(\"restore\", restore); } shared_ptr<Config> Config::copy() { auto that = make_shared<Config>(); that >class_num = class_num; that >embed_dim = embed_dim; that >hidden_dim = hidden_dim; that >vocab_size = vocab_size; that >window = window; that >preanal = preanal; that >errpatch = errpatch; that >restore = restore; return that; } } // namespace khaiii 512 [1398, 62, 22510, 18125, 11525, 62, 27740, 796, 474, 16184, 13, 8367, 7203, 20521, 62, 27740, 1600, 11525, 62, 27740, 1776, 611, 357, 20521, 62, 27740, 19841, 657, 8, 3714, 18181, 7, 69, 16762, 3712, 18982, 7203, 259, 12102, 705, 20521, 62, 27740, 6, 1988, 25, 23884, 1600, 11525, 62, 27740, 18125, 7104, 62, 27740, 796, 474, 16184, 13, 8367, 7203, 30342, 62, 27740, 1600, 7104, 62, 27740, 1776, 611, 357, 30342, 62, 27740, 19841, 657, 8, 3714, 18181, 7, 69, 16762, 3712, 18982, 7203, 259, 12102, 705, 30342, 62, 27740, 6, 1988, 25, 23884, 1600, 7104, 62, 27740, 18125, 12776, 397, 62, 7857, 796, 474, 16184, 13, 8367, 7203, 18893, 397, 62, 7857, 1600, 12776, 397, 62, 7857, 1776, 611, 357, 18893, 397, 62, 7857, 19841, 657, 8, 3714, 18181, 7, 69, 16762, 3712, 18982, 7203, 259, 12102, 705, 18893, 397, 62, 7857, 6, 1988, 25, 23884, 1600, 12776, 397, 62, 7857, 18125, 4324, 796, 474, 16184, 13, 8367, 7203, 17497, 1600, 4324, 1776, 611, 357, 17497, 19841, 657, 8, 3714, 18181, 7, 69, 16762, 3712, 18982, 7203, 259, 12102, 705, 17497, 6, 1988, 25, 23884, 1600, 4324, 18125, 20957, 62, 30814, 7, 8457, 77, 1776, 1782, 7951, 17056, 3712, 2502, 13154, 62, 30814, 7, 9979, 299, 75, 1219, 9038, 3712, 17752, 5, 474, 16184, 8, 1391, 662, 272, 282, 796, 474, 16184, 13, 8367, 7203, 3866, 272, 282, 1600, 662, 272, 282, 1776, 11454, 17147, 796, 474, 16184, 13, 8367, 7203, 8056, 17147, 1600, 11454, 17147, 1776, 11169, 796, 474, 16184, 13, 8367, 7203, 2118, 382, 1600, 11169, 1776, 1782, 4888, 62, 20692, 27, 16934, 29, 17056, 3712, 30073, 3419, 1391, 8295, 326, 796, 787, 62, 28710, 27, 16934, 29, 9783, 326, 1875, 4871, 62, 22510, 796, 1398, 62, 22510, 26, 326, 1875, 20521, 62, 27740, 796, 11525, 62, 27740, 26, 326, 1875, 30342, 62, 27740, 796, 7104, 62, 27740, 26, 326, 1875, 18893, 397, 62, 7857, 796, 12776, 397, 62, 7857, 26, 326, 1875, 17497, 796, 4324, 26, 326, 1875, 3866, 272, 282, 796, 662, 272, 282, 26, 326, 1875, 8056, 17147, 796, 11454, 17147, 26, 326, 1875, 2118, 382, 796, 11169, 26, 1441, 326, 26, 1782, 1782, 3373, 25745, 44081, 1872, 4178]\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "test = \"\"\" AUTHOR OF “A DICTIONARY OF AMERICAN AUTHORS,” “THE\n",
    "             STORY OF JANE AUSTEN’S LIFE,” “SICUT PATRIBUS\n",
    "                AND OTHER VERSE,” ETC.; AMERICAN EDITOR\n",
    "                    OF THE HENRY IRVING SHAKESPEARE,\n",
    "                                  ETC.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        [Illustration: Colophon\n",
    "                                  LEGE\n",
    "                                  QUID\n",
    "                                 LEGAS]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                 BOSTON\n",
    "                       Sherman, French & Company\n",
    "                                  1909\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                             Copyright 1909\n",
    "                       SHERMAN, FRENCH & COMPANY\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                 TO THE\n",
    "\n",
    "                 OLD CAMBRIDGE SHAKESPEARE ASSOCIATION\n",
    "\n",
    "                                  THIS\n",
    "\n",
    "                             LITTLE VOLUME\n",
    "\n",
    "                                   IS\n",
    "\n",
    "                          GRATEFULLY INSCRIBED\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                             PREFATORY NOTE\n",
    "\n",
    "\n",
    "  The Sixth Act of _The Merchant of Venice_ was first printed in the\n",
    "  _Cornhill Booklet_ for March, 1903. The _Shakespearean Fantasy_ now\n",
    "  appears for the first time in print.\n",
    "\n",
    "\n",
    "I am already far north of London, and as I walk in the streets of\n",
    "Petersburgh, I feel a cold northern breeze play upon my cheeks, which\n",
    "braces my nerves and fills me with delight. Do you understand this\n",
    "feeling? This breeze, which has travelled from the regions towards\n",
    "which I am advancing, gives me a foretaste of those icy climes.\n",
    "Inspirited by this wind of promise, my daydreams become more fervent\n",
    "and vivid. I try in vain to be persuaded that the pole is the seat of\n",
    "frost and desolation; it ever presents itself to my imagination as the\n",
    "region of beauty and delight. There, Margaret, the sun is for ever\n",
    "visible, its broad disk just skirting the horizon and diffusing a\n",
    "perpetual splendour. There—for with your leave, my sister, I will put\n",
    "some trust in preceding navigators—there snow and frost are banished;\n",
    "and, sailing over a calm sea, we may be wafted to a land surpassing in\n",
    "wonders and in beauty every region hitherto discovered on the habitable\n",
    "globe. Its productions and features may be without example, as the\n",
    "phenomena of the heavenly bodies undoubtedly are in those undiscovered\n",
    "solitudes. What may not be expected in a country of eternal light? I\n",
    "may there discover the wondrous power which attracts the needle and may\n",
    "regulate a thousand celestial observations that require only this\n",
    "voyage to render their seeming eccentricities consistent for ever. I\n",
    "shall satiate my ardent curiosity with the sight of a part of the world\n",
    "never before visited, and may tread a land never before imprinted by\n",
    "the foot of man. These are my enticements, and they are sufficient to\n",
    "conquer all fear of danger or death and to induce me to commence this\n",
    "laborious voyage with the joy a child feels when he embarks in a little\n",
    "boat, with his holiday mates, on an expedition of discovery up his\n",
    "native river. But supposing all these conjectures to be false, you\n",
    "cannot contest the inestimable benefit which I shall confer on all\n",
    "mankind, to the last generation, by discovering a passage near the pole\n",
    "to those countries, to reach which at present so many months are\n",
    "requisite; or by ascertaining the secret of the magnet, which, if at\n",
    "all possible, can only be effected by an undertaking such as mine.\n",
    "\n",
    "These reflections have dispelled the agitation with which I began my\n",
    "letter, and I feel my heart glow with an enthusiasm which elevates me\n",
    "to heaven, for nothing contributes so much to tranquillise the mind as\n",
    "a steady purpose—a point on which the soul may fix its intellectual\n",
    "eye. This expedition has been the favourite dream of my early years. I\n",
    "have read with ardour the accounts of the various voyages which have\n",
    "been made in the prospect of arriving at the North Pacific Ocean\n",
    "through the seas which surround the pole. You may remember that a\n",
    "history of all the voyages made for purposes of discovery composed the\n",
    "whole of our good Uncle Thomas’ library. My education was neglected,\n",
    "yet I was passionately fond of reading. These volumes were my study\n",
    "day and night, and my familiarity with them increased that regret which\n",
    "I had felt, as a child, on learning that my father’s dying injunction\n",
    "had forbidden my uncle to allow me to embark in a seafaring life.\n",
    "\"\"\" + ''.join(['hello ' for _ in range(1024)]) + \"\"\"check that they are OK. Assume that we have modified a typo in the file ~/d2l en/chapter_appendix tools for deep learning/ contributing.md. You can then check which files you have changed. At this point Git will prompt that the chapter_appendix tools for deep learning/ contributing.md file has been modified. mylaptop:d2l en me$ git status On branch master Your branch is up to date with 'origin/master'. Changes not staged for commit: (use \\\"git add <file>...\\\" to update what will be committed) (use \\\"git checkout <file>...\\\" to discard changes in working directory) modified: chapter_appendix tools for deep learning/contributing.md After confirming that this is what you want, execute the following command: git add chapter_appendix tools for deep learning/contributing.md git commit m 'Fix a typo in git documentation' git push The changed code will then be in your personal fork of the repository. To request the addition of your change, you have to create a pull request for the official repository of the book. Submitting Pull Requests As shown in Fig. B.6, go to your fork of the repository on GitHub and select New pull request. This will open up a screen that shows you the changes between your edits and what is current in the main repository of the book. New pull request. 1060 tFig. B.7 316 317 Tools for Deep Learning Finally, submit a pull request by clicking the button as shown in Fig. B.7. Make sure to describe the changes you have made in the pull request. This will make it easier for the authors to review it and to merge it with the book. Depending on the changes, this might get accepted right away, rejected, or more likely, you will get some feedback on the changes. Once you have incorporated them, you are good to go. Create pull request. B.6.4 Summary (cid:15) You can use GitHub to contribute to this book. (cid:15) You can edit the file on GitHub directly for minor changes. (cid:15) For a major change, please fork the repository, edit things locally, and only\"\"\" +  \"\"\"/**\n",
    " * @author  Jamie (jamie.lim@kakaocorp.com)\n",
    " * @copyright  Copyright (C) 2018-, Kakao Corp. All rights reserved.\n",
    " */\n",
    "\n",
    "\n",
    "#include \"khaiii/Config.hpp\"\n",
    "\n",
    "\n",
    "//////////////\n",
    "// includes //\n",
    "//////////////\n",
    "#include <fstream>\n",
    "\n",
    "#include \"fmt/format.h\"\n",
    "#include \"nlohmann/json.hpp\"\n",
    "\n",
    "#include \"khaiii/KhaiiiApi.hpp\"\n",
    "\n",
    "\n",
    "namespace khaiii {\n",
    "\n",
    "\n",
    "using std::exception;\n",
    "using std::ifstream;\n",
    "using std::make_shared;\n",
    "using std::shared_ptr;\n",
    "using std::string;\n",
    "\n",
    "\n",
    "/////////////\n",
    "// methods //\n",
    "/////////////\n",
    "void Config::read_from_file(string path) {\n",
    "    try {\n",
    "        ifstream ifs(path);\n",
    "        nlohmann::json jsn;\n",
    "        ifs >> jsn;\n",
    "        set_members(jsn);\n",
    "    } catch (const exception& exc) {\n",
    "        throw Except(fmt::format(\"fail to parse config: {}\", exc.what()));\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "void Config::override_from_str(const char* opt_str) {\n",
    "    if (opt_str == nullptr || opt_str[0] == '\\0') return;\n",
    "\n",
    "    try {\n",
    "        auto jsn = nlohmann::json::parse(opt_str);\n",
    "        override_members(jsn);\n",
    "    } catch (const exception& exc) {\n",
    "        throw Except(fmt::format(\"fail to parse option: {}\\n{}\", exc.what(), opt_str));\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "Config* Config::copy_and_override(const char* opt_str) {\n",
    "    if (opt_str == nullptr || opt_str[0] == '\\0') return this;\n",
    "\n",
    "    auto found = _cfg_cache.find(opt_str);\n",
    "    if (found != _cfg_cache.end()) return found->second.get();\n",
    "\n",
    "    auto cfg = copy();\n",
    "    try {\n",
    "        auto jsn = nlohmann::json::parse(opt_str);\n",
    "        cfg->override_members(jsn);\n",
    "        _cfg_cache[opt_str] = cfg;\n",
    "    } catch (const exception& exc) {\n",
    "        throw Except(fmt::format(\"fail to parse option: {}\\n{}\", exc.what(), opt_str));\n",
    "    }\n",
    "\n",
    "    return cfg.get();\n",
    "}\n",
    "\n",
    "\n",
    "void Config::set_members(const nlohmann::json& jsn) {\n",
    "    class_num = jsn.value(\"class_num\", class_num);\n",
    "    if (class_num <= 0) throw Except(fmt::format(\"invalid 'class_num' value: {}\", class_num));\n",
    "\n",
    "    embed_dim = jsn.value(\"embed_dim\", embed_dim);\n",
    "    if (embed_dim <= 0) throw Except(fmt::format(\"invalid 'embed_dim' value: {}\", embed_dim));\n",
    "\n",
    "    hidden_dim = jsn.value(\"hidden_dim\", hidden_dim);\n",
    "    if (hidden_dim <= 0) throw Except(fmt::format(\"invalid 'hidden_dim' value: {}\", hidden_dim));\n",
    "\n",
    "    vocab_size = jsn.value(\"vocab_size\", vocab_size);\n",
    "    if (vocab_size <= 0) throw Except(fmt::format(\"invalid 'vocab_size' value: {}\", vocab_size));\n",
    "\n",
    "    window = jsn.value(\"window\", window);\n",
    "    if (window <= 0) throw Except(fmt::format(\"invalid 'window' value: {}\", window));\n",
    "\n",
    "    override_members(jsn);\n",
    "}\n",
    "\n",
    "void Config::override_members(const nlohmann::json& jsn) {\n",
    "    preanal = jsn.value(\"preanal\", preanal);\n",
    "    errpatch = jsn.value(\"errpatch\", errpatch);\n",
    "    restore = jsn.value(\"restore\", restore);\n",
    "}\n",
    "\n",
    "shared_ptr<Config> Config::copy() {\n",
    "    auto that = make_shared<Config>();\n",
    "    that->class_num = class_num;\n",
    "    that->embed_dim = embed_dim;\n",
    "    that->hidden_dim = hidden_dim;\n",
    "    that->vocab_size = vocab_size;\n",
    "    that->window = window;\n",
    "    that->preanal = preanal;\n",
    "    that->errpatch = errpatch;\n",
    "    that->restore = restore;\n",
    "    return that;\n",
    "}\n",
    "\n",
    "\n",
    "}    // namespace khaiii\"\"\"\n",
    "\n",
    "\n",
    "chunks = split_into_sentences(test)\n",
    "tokenizer = get_tokenizer()\n",
    "batch_result = tokenizer.batch_encode_plus(chunks, max_length=512, return_overflowing_tokens=True, truncation=True)\n",
    "for n, (ids, ovf) in enumerate(zip(batch_result[\"input_ids\"], batch_result[\"overflowing_tokens\"])):\n",
    "    print(chunks[n], len(ids), ovf)\n",
    "\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from unstructured.cleaners.core import clean\n",
    "from transformers import PreTrainedTokenizer,GPT2Tokenizer\n",
    "from typing import Any\n",
    "\n",
    "class SentenceChunker:\n",
    "    \n",
    "\n",
    "    def _split_into_sentences(self, text):\n",
    "        # Regex pattern to split on sentence-ending punctuation followed by space or end of string\n",
    "        clean_text = clean(text, extra_whitespace=True, dashes=True, bullets=True)\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', clean_text)\n",
    "        return [sentence for sentence in sentences]\n",
    "    \n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, max_length:int) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, batch, *args: Any, **kwds: Any) -> Any:\n",
    "        if isinstance(batch, str):\n",
    "            batch = [batch]\n",
    "        batch_of_chunks = [self._split_into_sentences(seq) for seq in batch]\n",
    "        batch_of_encodings = [self.tokenizer.batch_encode_plus(chunks, return_length=True) for chunks in batch_of_chunks]\n",
    "\n",
    "        result = {\"success\": [], \"failure\": []}\n",
    "        success_batch_bucket = []\n",
    "        failure_batch_bucket = []\n",
    "        for bi, encodings in enumerate(batch_of_encodings):\n",
    "            bucket = []\n",
    "            tokens_total = 0\n",
    "            for n, token_count in enumerate(encodings[\"length\"]):\n",
    "                if token_count > self.max_length:\n",
    "                    failure_batch_bucket.append({\"text\":batch_of_chunks[bi][n], \"length\": token_count})\n",
    "                    continue\n",
    "                if token_count + tokens_total > self.max_length:\n",
    "                    # bucket is full\n",
    "                    success_batch_bucket.append({\"text\":' '.join(bucket), \"length\": tokens_total})\n",
    "                    bucket.clear()\n",
    "                    tokens_total = 0\n",
    "                    \n",
    "                bucket.append(batch_of_chunks[bi][n])\n",
    "                tokens_total += token_count\n",
    "            result[\"success\"].append([*success_batch_bucket])\n",
    "            result['failure'].append([*failure_batch_bucket])\n",
    "            success_batch_bucket.clear()\n",
    "            failure_batch_bucket.clear()\n",
    "        return result\n",
    "                \n",
    "\n",
    "\n",
    "def get_tokenizer() -> PreTrainedTokenizer:\n",
    "    \n",
    "    tokenizer: PreTrainedTokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"<pad>\", \"bos_token\":\"<s>\", \"eos_token\":\"</s>\"})\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer\u001b[49m()\n\u001b[1;32m      2\u001b[0m chunker \u001b[38;5;241m=\u001b[39m SentenceChunker(tokenizer, \u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m      3\u001b[0m result \u001b[38;5;241m=\u001b[39m chunker(test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "chunker = SentenceChunker(tokenizer, 512)\n",
    "result = chunker(test)\n",
    "for c in result[\"success\"][0]:\n",
    "    print(f\"- {c}\\n\")\n",
    "\n",
    "result[\"failure\"]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 19/19 [00:00<00:00, 176309.24it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "\n",
    "dataset = load_dataset('wikimedia/wikisource', '20231201.en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "chunker = SentenceChunker(tokenizer, 512)\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "n_dataset = train_dataset.map(lambda x: chunker(x[\"text\"]), batched=True, num_proc=15).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "success = n_dataset.select_columns(['success']).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1031594 examples [00:09, 109691.18 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['length', 'text'],\n",
       "    num_rows: 1031594\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from typing import Any\n",
    "from datasets import Dataset, Features, Value\n",
    "import pyarrow\n",
    "# def extract_success(dataset: Dataset):\n",
    "#             for batch in dataset[\"success\"]:\n",
    "#                 for seq in batch:\n",
    "#                     yield seq\n",
    "\n",
    "class SuccessCaseGenerator:\n",
    "    def __init__(self, dataset: Dataset) -> None:\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __call__(self, *args: Any, **kwds: Any) -> Any:\n",
    "        for b in self.dataset[\"success\"]:\n",
    "            for seq in b:\n",
    "                yield seq\n",
    "\n",
    "Dataset.from_generator(SuccessCaseGenerator(success))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# paper wrapup - [Attention is All you need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "## Problem\n",
    "\n",
    "### LSTM, GRU don't scale well\n",
    "\n",
    "- 기존의 Sequence Modeling을 위해 사용되던 LSTM, GRU는 재귀적 특성(Sequence)으로 인해 병렬화에 제약을 지님\n",
    "- 이러한 Sequence Length의 증가에 따른 Computation & Memory Complexity를 해결하기 위해 Convolution 기반의 방법 등이 시도되었으나 Long distance의 의존성을 제대로 다룰 수 없었음\n",
    "  - 예를 임의의 두 위치의 거리에 따라서 \n",
    "  - ConvS2S는 O(n)\n",
    "  - ByteNet는 O(logN)\n",
    "- 즉, long distance의 의존성을 적절하게 다룰 수 있으면서 동시에 sequence에 따른 복잡도를 최소화 할 수 있는 방법의 필요\n",
    "\n",
    "### Transformers\n",
    "\n",
    "- Transformer는 두 지점의 거리에 대해서 O(1)의 복잡도를 제공 (Generation의 경우 예외)\n",
    "- Self-Attention은 Reading Comprehension, Semantic Representation, 등에 뛰어난 성능을 보였음\n",
    "- Transfomer는 RNN이나 CNN 등을 사용하지 않고 순수하게 Self-Attention만을 사용한 최초의 사례\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "Encoder-Decoder Architecture\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "- 6 identical layers\n",
    "- each layer\n",
    "  - multi-head self-attention (w/ residual connection)\n",
    "  - position-wise feedforward (w/ residual connection)\n",
    "\n",
    "```python\n",
    "\n",
    "layer_1 = LayerNorm(MultiHeadSelfAttention(x) + x)\n",
    "layer_2 = LayerNorm(PositionWiseFeedforward(x) + x)\n",
    "\n",
    "```\n",
    "\n",
    "#### Decoder\n",
    "\n",
    "- 6 identical layers\n",
    "- each layer\n",
    "  - multi-head self-attention (w/ residual connection)\n",
    "  - multi-head attention (w/ residual connection)\n",
    "  - position-wise feedforward (w/residual connection)\n",
    "\n",
    "```python\n",
    "\n",
    "layer_1 = LayerNorm(MultiHeadAttention(x) + x)\n",
    "layer_2 = LayerNorm(MultiHeadAttention(qk: encoder_output, v: x))\n",
    "layer_3 = LayerNorm(PositionWiseFeedforward(x) + x)\n",
    "\n",
    "```\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled dot-product attention\n",
    "\n",
    "- Attention의 주요 알고리즘으로 Scaled dot-product attention을 사용\n",
    "- dot-product\n",
    "  - attention을 구하는 연산으로 Mat Mul을 사용\n",
    "- scaled\n",
    "  - dimension of key and value. 즉, word embedding vector의 dimension dk\n",
    "  - 1/sqrt(dk)로 attension을 scaling\n",
    "\n",
    "\n",
    "```python\n",
    "ScaledDotProduct(Q,K,V) = Softmax(Q@K.T / sqrt(dk))@V\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ScaledDotProductAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, device=None, dtype: torch.dtype=torch.float, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.dk = torch.sqrt(torch.scalar_tensor(d_model, device=device, dtype=dtype))\n",
    "    \n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        # input should have (B,N,d_model)\n",
    "        # q (b,1,d_model) , k (b,n,d_model)\n",
    "        # qk = (b,1,n)\n",
    "        scaled_qk = q@torch.transpose(k, 2, 1) / self.dk\n",
    "        if mask is not None:\n",
    "            scaled_qk = scaled_qk * mask\n",
    "        attention_weights = torch.softmax(scaled_qk, dim=-1)\n",
    "        return  attention_weights @  v\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head attention\n",
    "\n",
    "- d_model 즉, Query, Key ,Value의 word embedding vector를 다수의 sub vector로 나누어서 각 sub vector를 입력으로 하는 scaled dot-product attention 다수를 조합하여 하나의 Attention block을 구성함.\n",
    "\n",
    "```python\n",
    "MultiHeadAttention(Q,K,V,n_heads) = concat(*[ScaledDotProduct(linear(qi),linear(ki),linear(vi)) for qi,ki,vi in zip(Q.split(n_heads), K.split(n_heads), V.split(n_heads))])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, n_head, device=None, dtype: torch.dtype=torch.float, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.n_head = n_head\n",
    "        self.depth = d_model // n_head\n",
    "\n",
    "        self.q_linear = torch.nn.Linear(d_model, d_model, device=device, dtype=dtype)\n",
    "        self.k_linear = torch.nn.Linear(d_model, d_model, device=device, dtype=dtype)\n",
    "        self.v_linear = torch.nn.Linear(d_model, d_model, device=device, dtype=dtype)\n",
    "\n",
    "        self.attns = torch.nn.ModuleList([ScaledDotProductAttention(d_model=self.depth, device=device, dtype=dtype) for _ in range(n_head)])\n",
    "        self.output_linear = torch.nn.Linear(d_model, d_model, device=device, dtype=dtype)\n",
    "                \n",
    "        \n",
    "    def forward(self, input: torch.Tensor, mask:torch.Tensor=None) -> torch.Tensor:\n",
    "        if len(input.shape) == 2:\n",
    "            input = input.unsqueeze(0)\n",
    "        if len(input.shape) != 3:\n",
    "            raise ValueError(f'unsupported tensor shape: {input.shape}, should be form of (B,N,d)')\n",
    "        \n",
    "        b,n,d = input.shape\n",
    "        q = self.q_linear.forward(input).view((b, n, self.n_head, -1))\n",
    "        k = self.k_linear.forward(input).view((b, n, self.n_head, -1))\n",
    "        v = self.v_linear.forward(input).view((b, n, self.n_head, -1))\n",
    "        attn_output = torch.concat([self.attns[i].forward(q[:,:,i,:].view((b,n,self.depth)), \n",
    "                                            k[:,:,i,:].view((b,n, self.depth)), \n",
    "                                            v[:,:,i,:].view((b,n, self.depth)), mask) for i in range(self.n_head)],dim=-1)\n",
    "        return self.output_linear.forward(attn_output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "test = torch.rand((1, 1024, 512), device=device, dtype=torch.float16)\n",
    "\n",
    "attention = MultiHeadAttention(512, 8, device=device, dtype=torch.float16)\n",
    "attention = torch.compile(attention)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 512])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = attention.forward(test)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position-wise feedforward\n",
    "\n",
    "- Linear - Relu - Linear\n",
    "\n",
    "\n",
    "## Transformer Layer\n",
    "\n",
    "- GPT논문의 Transformer layer는 Attention is all you need의 transformer는 Encoder 유사한 구조로 residual connection을 갖는 Multi-head attention과 position wise feedforward의 2개의 sublayer로 구성되어 있고 각 sublayer의 출력에 layer norm이 추가되는 형태\n",
    "- 첫 GPT 논문에서는 이러한 transformer block 12개를 쌓아 model을 구성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class PositionWiseFeedforward(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model:int, device, dtype: torch.dtype=torch.float, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.pwff = torch.nn.Sequential(torch.nn.Linear(d_model, d_model * 4, device=device,dtype=dtype), \n",
    "                                            torch.nn.GELU(), \n",
    "                                            torch.nn.Linear(4* d_model, d_model, device=device, dtype=dtype))\n",
    "        \n",
    "    def forward(self, input: torch.Tensor)-> torch.Tensor:\n",
    "        return self.pwff.forward(input)\n",
    "\n",
    "\n",
    "class Transformer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_head, d_model, device, dtype:torch.dtype=torch.float, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.input_norm = torch.nn.LayerNorm(d_model, device=device, dtype=dtype)\n",
    "        self.mha = MultiHeadAttention(d_model=d_model, n_head=n_head, device=device, dtype=dtype)\n",
    "        self.mha_lnorm = torch.nn.LayerNorm(d_model, device=device,dtype=dtype)\n",
    "        self.pw_ff = PositionWiseFeedforward(d_model=d_model, device=device, dtype=dtype)\n",
    "\n",
    "    def forward(self, input:torch.Tensor, mask: torch.Tensor=None) -> torch.Tensor:\n",
    "        # Pre-LayerNormalization from GPT-3, (note: Post-LayerNormalization is used for GPT-2 and original paper)\n",
    "        norm_input = self.input_norm.forward(input)\n",
    "        mha_output = input + self.mha.forward(norm_input, mask)\n",
    "        norm_mha_output = self.mha_lnorm(mha_output)\n",
    "        return mha_output + self.pw_ff.forward(norm_mha_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 512])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "transformer = Transformer(8, 512, device=device, dtype=torch.float16)\n",
    "output = transformer.forward(test)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding & Tokenization\n",
    "\n",
    "- Text를 적절한 Numerical Representation으로 변형을 하기 위해서는 크게 2가지의 처리 단계를 필요로 하는데 이는 Tokenization과 Embedding이다.\n",
    "\n",
    "\n",
    "### Tokenization\n",
    "- Text를 최소 단위로 쪼개어 각 의미 단위에 고유한 ID를 부여하는 것.\n",
    "- 최소 단위로 쪼개는 방식에 따라 Character Level Tokenization 부터 Subword Tokenization 등 다양한 방법이 있다. \n",
    "- 단, 신규 어휘의 확장에 유연하게 대응할 수 있는 장점을 지닌 Subword 방식을 많이 쓰고 있으며 BPE라는 방식이 유명하다.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 64, 50])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "vocab_size = 500\n",
    "dense_rep = 50\n",
    "batch = 8\n",
    "seq = 64\n",
    "\n",
    "input = torch.ones((batch, seq), dtype=torch.int) ## test input\n",
    "\n",
    "embedding = torch.nn.Embedding(vocab_size, dense_rep) ## convert the token_id to dense vector\n",
    "\n",
    "output = embedding.forward(input=input)\n",
    "\n",
    "output.shape # would be (8,64,50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from typing import Any\n",
    "from lightning.pytorch.utilities.types import STEP_OUTPUT\n",
    "\n",
    "class ToyGPT(L.LightningModule):\n",
    "\n",
    "    def __init__(self, \n",
    "                 vocab_size:int, \n",
    "                 d_model:int, n_head:int, num_layers:int, pad_id:int=None,  device=None, dtype:torch.dtype=torch.float, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(vocab_size, d_model, padding_idx=pad_id, device=device, dtype=dtype)\n",
    "        self.transformers = torch.nn.Sequential(*[Transformer(n_head=n_head, d_model=d_model, device=device, dtype=dtype) for _ in range(num_layers)])\n",
    "        self.output_linear = torch.nn.Linear(d_model, vocab_size, device=device, dtype=dtype)\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        # X should have shape of (B,N)\n",
    "        if len(X.shape) == 1:\n",
    "            X = X.unsqueeze(0)\n",
    "\n",
    "        return self.softmax.forward(self.output_linear.forward(self.transformers.forward(self.embedding.forward(input=X))))\n",
    "    \n",
    "\n",
    "    def training_step(self, batch_input, batch_index, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\n",
    "\n",
    "        return super().training_step(*args, **kwargs)\n",
    "    \n",
    "    def test_step(self, batch_input, batch_index, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\n",
    "        return super().test_step(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.9264e-04, 2.3949e-04, 2.5058e-04,  ..., 2.1100e-04,\n",
       "          1.8418e-04, 1.1563e-04],\n",
       "         [3.1519e-04, 4.2987e-04, 2.3699e-04,  ..., 7.5996e-05,\n",
       "          4.4417e-04, 1.0359e-04],\n",
       "         [1.1337e-04, 2.0766e-04, 1.7440e-04,  ..., 7.3850e-05,\n",
       "          1.2201e-04, 5.0604e-05],\n",
       "         ...,\n",
       "         [1.1820e-04, 1.6356e-04, 3.3689e-04,  ..., 9.8586e-05,\n",
       "          2.1982e-04, 8.1062e-05],\n",
       "         [1.9264e-04, 2.3949e-04, 2.5058e-04,  ..., 2.1100e-04,\n",
       "          1.8418e-04, 1.1563e-04],\n",
       "         [1.9264e-04, 2.3949e-04, 2.5058e-04,  ..., 2.1100e-04,\n",
       "          1.8418e-04, 1.1563e-04]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "test_input = torch.tensor([0,1,2,3,4,0,0], dtype=torch.int, device=device)\n",
    "\n",
    "gpt = ToyGPT(5000, 128, n_head=8, num_layers=4, pad_id=0, device=device, dtype=torch.float16)\n",
    "output = gpt.forward(test_input)\n",
    "output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Masking` in Masked Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example sequence including special tokens\n",
    "sequence = [\"<bos>\", \"hello\", \"there\", \"I'm\", \"david\", \"<eos>\"]\n",
    "seq_length = len(sequence)\n",
    "\n",
    "# Creating a mask with large negative numbers for visualization\n",
    "filled_mask = np.triu(np.ones((seq_length, seq_length)), k=1) * -10\n",
    "\n",
    "# Plotting the filled mask\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(filled_mask, cmap='viridis')\n",
    "plt.colorbar(label='Mask Value')\n",
    "plt.title('Filled Attention Mask Visualization for \"hello there I\\'m david\"')\n",
    "plt.xlabel('Key Tokens')\n",
    "plt.ylabel('Query Tokens')\n",
    "plt.xticks(ticks=range(seq_length), labels=sequence)\n",
    "plt.yticks(ticks=range(seq_length), labels=sequence)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Masking은 Scaled(Q@K.T)의 결과인 Attention Score에서 QnKm (where m > n)에 있는 값들을 -inf으로 변환한다.\n",
    "- 이는 GPT와 같은 Autoregressive 모델의 주요한 목적, 즉, Sequence에서 한쪽의 정보를 기반으로 그 다음에 올 정보를 추정해야하는 모델에서 \n",
    "- 추정의 정답 즉, Ground Truth에 대한 정보가 Training에 포함되는 것을 막기 위함이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "- Pre-trained GPT2 tokenizer will be used\n",
    "- Special Tokens\n",
    "  - pad_token: `<pad>`\n",
    "  - bos_token: `<s>`\n",
    "  - eos_token: `</s>`\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2Tokenizer(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizer, GPT2Tokenizer\n",
    "\n",
    "def get_tokenizer() -> PreTrainedTokenizer:\n",
    "    \n",
    "    tokenizer: PreTrainedTokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"<pad>\", \"bos_token\":\"<s>\", \"eos_token\":\"</s>\"}) # special \n",
    "    return tokenizer\n",
    "\n",
    "get_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "- \"wikimedia/wikisource\", \"20231201.en\"\n",
    "- HuggingFace의 Datasets와 Unstructured.io를 이용 Data의 download, pre-processing을 구현\n",
    "- Lightening framework과 연동을 위해 LighteningDataModule을 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Dict\n",
    "\n",
    "import lightning as L\n",
    "from datasets import Dataset, load_from_disk, load_dataset\n",
    "from lightning.pytorch.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS\n",
    "from transformers import PreTrainedTokenizer\n",
    "from torch.utils import data\n",
    "from unstructured.cleaners.core import (\n",
    "    replace_unicode_quotes, clean, clean_ligatures\n",
    ")\n",
    "import re\n",
    "from unstructured.cleaners.core import clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Pre-processing\n",
    "- Text Cleansing\n",
    "  - extra whitespace 제거\n",
    "  - unicode quote 대체\n",
    "  - ligatures 제거\n",
    "- 이렇게 Cleansing된 Text를 완전한 Sentence 단위로 분절하여 Max Length에 맞도록 전처리하여 Local에 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceChunker:\n",
    "    \"\"\"\n",
    "    A class responsible for chunking text into sentences and tokenizing them\n",
    "    according to a specified maximum length.\n",
    "\n",
    "    Attributes:\n",
    "        tokenizer (PreTrainedTokenizer): A tokenizer from the transformers library\n",
    "                                         used for tokenizing sentences.\n",
    "        max_length (int): The maximum token length for a single chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    def _split_into_sentences(self, text):\n",
    "        \"\"\"\n",
    "        Splits the input text into sentences.\n",
    "\n",
    "        The text is first cleaned to standardize it (removing extra whitespaces, \n",
    "        replacing unicode quotes, and removing ligatures). Then, it is split into \n",
    "        sentences using a regular expression that looks for sentence end markers \n",
    "        (., !, ?) followed by a whitespace.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be split into sentences.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of sentences extracted from the input text.\n",
    "        \"\"\"\n",
    "        # Clean the text and split it into sentences\n",
    "        clean_text = replace_unicode_quotes(clean_ligatures(clean(text, extra_whitespace=True)))\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', clean_text)\n",
    "        return [sentence for sentence in sentences]\n",
    "\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, max_length:int) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the SentenceChunker with a tokenizer and a maximum length.\n",
    "\n",
    "        Args:\n",
    "            tokenizer (PreTrainedTokenizer): The tokenizer to be used for tokenization.\n",
    "            max_length (int): The maximum token length for a single chunk.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, batch, *args: Any, **kwds: Any) -> Any:\n",
    "        \"\"\"\n",
    "        Processes a batch of text sequences by first splitting them into sentences,\n",
    "        then encoding each sentence. The sentences are then chunked according to the \n",
    "        maximum length, ensuring no chunk exceeds this limit.\n",
    "\n",
    "        Args:\n",
    "            batch: A batch of text sequences.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, List]: A dictionary with two keys, 'success' and 'failure'.\n",
    "                             'success' contains chunks that are within the max_length,\n",
    "                             'failure' contains chunks that exceed the max_length.\n",
    "        \"\"\"\n",
    "        # Handle single string inputs by wrapping them in a list\n",
    "        if isinstance(batch, str):\n",
    "            batch = [batch]\n",
    "\n",
    "        # Split each sequence in the batch into sentences and encode them\n",
    "        batch_of_chunks = [self._split_into_sentences(seq) for seq in batch]\n",
    "        batch_of_encodings = [self.tokenizer.batch_encode_plus(chunks, return_length=True) for chunks in batch_of_chunks]\n",
    "\n",
    "        result = {\"success\": [], \"failure\": []}\n",
    "        success_batch_bucket = []\n",
    "        failure_batch_bucket = []\n",
    "\n",
    "        # Iterate over each sequence's encodings and chunk them\n",
    "        for bi, encodings in enumerate(batch_of_encodings):\n",
    "            bucket = []\n",
    "            tokens_total = 0\n",
    "\n",
    "            # Process each sentence in the sequence\n",
    "            for n, token_count in enumerate(encodings[\"length\"]):\n",
    "                # Handle sentences that exceed the max length\n",
    "                if token_count > self.max_length:\n",
    "                    failure_batch_bucket.append({\"text\":batch_of_chunks[bi][n], \"length\": token_count})\n",
    "                    continue\n",
    "\n",
    "                # Check if adding the sentence would exceed the max length\n",
    "                if token_count + tokens_total > self.max_length:\n",
    "                    # Current bucket is full, save and reset it\n",
    "                    success_batch_bucket.append({\"text\":' '.join(bucket), \"length\": tokens_total})\n",
    "                    bucket.clear()\n",
    "                    tokens_total = 0\n",
    "                \n",
    "                # Add the sentence to the current bucket\n",
    "                bucket.append(batch_of_chunks[bi][n])\n",
    "                tokens_total += token_count\n",
    "\n",
    "            # Append the processed batches to the result\n",
    "            result[\"success\"].append([*success_batch_bucket])\n",
    "            result['failure'].append([*failure_batch_bucket])\n",
    "            success_batch_bucket.clear()\n",
    "            failure_batch_bucket.clear()\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Loading\n",
    "- shuffling\n",
    "- batch tokenization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SuccessCaseGenerator:\n",
    "    def __init__(self, datasets: List[Dataset], transform=None) -> None:\n",
    "        self.datasets = datasets\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, *args: Any, **kwds: Any) -> Any:\n",
    "        for ds in self.datasets:\n",
    "            for b in ds[\"success\"]:\n",
    "                for seq in b:\n",
    "                    if self.transform:\n",
    "                        seq = self.transform(seq)\n",
    "                    yield seq\n",
    "\n",
    "\n",
    "class WikiSourceDataModule(L.LightningDataModule):\n",
    "\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, max_length:int, batch_size:int, languages:List[str]=['en'], train_size:float=0.9, num_proc=15) -> None:\n",
    "        super().__init__()\n",
    "        self.languages = languages\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.train_size = train_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_proc = num_proc\n",
    "\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "\n",
    "        def transform(v:Dict[str, Any]) -> str:\n",
    "            return {\"length\": v[\"length\"], \"text\": f\"<s>{v['text']}</s>\"}\n",
    "        \n",
    "        sentence_chunker = SentenceChunker(self.tokenizer, self.max_length - 2)\n",
    "        datasets = [load_dataset('wikimedia/wikisource', f\"20231201.{lang}\")[\"train\"].map(lambda b: sentence_chunker(b[\"text\"]), batched=True, num_proc=self.num_proc).flatten() for lang in self.languages]\n",
    "        success_ds = Dataset.from_generator(SuccessCaseGenerator(datasets, transform=transform))\n",
    "        success_ds = success_ds.train_test_split(test_size=(1 - self.train_size), train_size=self.train_size)\n",
    "        success_ds.save_to_disk('local_dscache')\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        self.dataset = load_from_disk('local_dscache')\n",
    "        self.train_dataset = self.dataset['train'].train_test_split(test_size=(1 - self.train_size), train_size=self.train_size)\n",
    "\n",
    "        return super().setup(stage)\n",
    "\n",
    "    def _tokenize(self, data):\n",
    "            inputs = data['text']\n",
    "            return self.tokenizer.batch_encode_plus(inputs, return_tensors=\"pt\", padding=True, return_length=True, max_length=self.max_length)\n",
    "    \n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        train_dataset = self.train_dataset[\"train\"].shuffle().map(self._tokenize, batched=True, batch_size=self.batch_size, num_proc=self.num_proc)\n",
    "        return data.DataLoader(train_dataset.with_format(type=\"torch\"))\n",
    "    \n",
    "    def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        val_dataset = self.train_dataset[\"test\"].shuffle().map(self._tokenize, batched=True, batch_size=self.batch_size, num_proc=self.num_proc)\n",
    "        return data.DataLoader(val_dataset.with_format(type=\"torch\"))\n",
    "    \n",
    "    def test_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        test_dataset = self.dataset[\"test\"].shuffle().map(self._tokenize, batched=True, batch_size=self.batch_size, num_proc=self.num_proc)\n",
    "        return data.DataLoader(test_dataset.with_format(type=\"torch\"))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 19/19 [00:00<00:00, 354185.67it/s]\n",
      "Saving the dataset (4/4 shards): 100%|██████████| 936713/936713 [00:07<00:00, 119153.60 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 104080/104080 [00:00<00:00, 134662.93 examples/s]\n",
      "Map (num_proc=15):   0%|          | 0/843041 [00:00<?, ? examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):   0%|          | 160/843041 [00:00<53:08, 264.33 examples/s] /home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):   0%|          | 572/843041 [00:01<17:58, 780.98 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):   0%|          | 1280/843041 [00:01<09:38, 1454.53 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):   0%|          | 2212/843041 [00:02<06:45, 2075.57 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):   0%|          | 3432/843041 [00:02<04:54, 2851.09 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):   1%|          | 4304/843041 [00:02<05:30, 2539.56 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):   1%|          | 6748/843041 [00:03<03:15, 4277.16 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):   1%|          | 8608/843041 [00:03<03:03, 4553.14 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):   1%|          | 10264/843041 [00:04<02:49, 4926.09 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):   1%|▏         | 12400/843041 [00:04<03:11, 4347.33 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):   2%|▏         | 13880/843041 [00:05<04:24, 3134.82 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):   2%|▏         | 17200/843041 [00:06<02:30, 5474.60 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):   2%|▏         | 19564/843041 [00:06<03:41, 3722.98 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):   3%|▎         | 28580/843041 [00:07<01:45, 7728.07 examples/s] /home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15): 100%|██████████| 843041/843041 [02:16<00:00, 6163.35 examples/s]\n",
      "Map (num_proc=15):   0%|          | 0/93672 [00:00<?, ? examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):   0%|          | 4/93672 [00:00<5:25:09,  4.80 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):   0%|          | 368/93672 [00:01<02:54, 535.00 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):   1%|          | 1028/93672 [00:01<01:11, 1304.83 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):   2%|▏         | 1952/93672 [00:02<00:45, 2000.02 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):   4%|▎         | 3316/93672 [00:02<00:30, 2952.88 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):   5%|▍         | 4680/93672 [00:03<00:23, 3719.13 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):   7%|▋         | 6592/93672 [00:03<00:20, 4147.09 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):   9%|▉         | 8568/93672 [00:03<00:16, 5088.87 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):  12%|█▏        | 11172/93672 [00:04<00:20, 4003.46 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):  15%|█▍        | 13636/93672 [00:05<00:24, 3213.38 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):  18%|█▊        | 16952/93672 [00:06<00:21, 3606.41 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):  22%|██▏       | 21060/93672 [00:06<00:14, 4922.43 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):  28%|██▊       | 26456/93672 [00:07<00:10, 6622.91 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):  31%|███▏      | 29480/93672 [00:08<00:10, 5977.95 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15): 100%|██████████| 93672/93672 [00:20<00:00, 4497.99 examples/s]\n",
      "Map (num_proc=15):   0%|          | 0/104080 [00:00<?, ? examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):   0%|          | 164/104080 [00:00<06:31, 265.40 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):   1%|          | 552/104080 [00:01<02:16, 757.64 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):   1%|          | 1004/104080 [00:01<01:25, 1198.70 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):   2%|▏         | 1820/104080 [00:02<00:58, 1739.89 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):   3%|▎         | 3184/104080 [00:02<00:35, 2852.85 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):   4%|▍         | 4048/104080 [00:02<00:42, 2338.13 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):   6%|▌         | 6416/104080 [00:03<00:23, 4072.84 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):   8%|▊         | 8716/104080 [00:04<00:19, 4780.87 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):  10%|▉         | 10120/104080 [00:04<00:28, 3315.08 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):  13%|█▎        | 13144/104080 [00:05<00:26, 3497.21 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):  16%|█▌        | 16320/104080 [00:05<00:19, 4479.87 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):  18%|█▊        | 19204/104080 [00:06<00:22, 3771.63 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):  24%|██▎       | 24648/104080 [00:07<00:13, 5677.65 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15):  28%|██▊       | 28752/104080 [00:08<00:11, 6541.52 examples/s]/home/fritzprix/miniconda3/envs/torch/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Map (num_proc=15): 100%|██████████| 104080/104080 [00:22<00:00, 4724.12 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from data import WikiSourceDataModule\n",
    "tokenizer = get_tokenizer()\n",
    "data = WikiSourceDataModule(tokenizer, 510, batch_size=4, languages=['en'])\n",
    "data.prepare_data()\n",
    "data.setup(\"fit\")\n",
    "train_data = data.train_dataloader()\n",
    "val_data = data.val_dataloader()\n",
    "test_data = data.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 843041/843041 [01:45<00:00, 7986.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training : 0.40628108382225037 billion tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93672/93672 [00:11<00:00, 7899.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation : 0.04514230415225029 billion tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104080/104080 [00:12<00:00, 8037.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test : 0.05013979598879814 billion tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_tokens_count(loader):\n",
    "    overall_token_count = 0\n",
    "    for data in tqdm(loader):\n",
    "        token_lengths = data['length']\n",
    "        overall_token_count += token_lengths.sum()\n",
    "\n",
    "    return overall_token_count\n",
    "\n",
    "print(f\"training : {get_tokens_count(train_data) / 1e9} billion tokens\")\n",
    "print(f\"validation : {get_tokens_count(val_data) / 1e9} billion tokens\")\n",
    "print(f\"test : {get_tokens_count(test_data) / 1e9} billion tokens\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

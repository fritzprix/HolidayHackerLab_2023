{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# paper wrapup - [Attention is All you need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "## Problem\n",
    "\n",
    "### LSTM, GRU don't scale well\n",
    "\n",
    "- 기존의 Sequence Modeling을 위해 사용되던 LSTM, GRU는 재귀적 특성(Sequence)으로 인해 병렬화에 제약을 지님\n",
    "- 이러한 Sequence Length의 증가에 따른 Computation & Memory Complexity를 해결하기 위해 Convolution 기반의 방법 등이 시도되었으나 Long distance의 의존성을 제대로 다룰 수 없었음\n",
    "  - 예를 임의의 두 위치의 거리에 따라서 \n",
    "  - ConvS2S는 O(n)\n",
    "  - ByteNet는 O(logN)\n",
    "- 즉, long distance의 의존성을 적절하게 다룰 수 있으면서 동시에 sequence에 따른 복잡도를 최소화 할 수 있는 방법의 필요\n",
    "\n",
    "### Transformers\n",
    "\n",
    "- Transformer는 두 지점의 거리에 대해서 O(1)의 복잡도를 제공 (Generation의 경우 예외)\n",
    "- Self-Attention은 Reading Comprehension, Semantic Representation, 등에 뛰어난 성능을 보였음\n",
    "- Transfomer는 RNN이나 CNN 등을 사용하지 않고 순수하게 Self-Attention만을 사용한 최초의 사례\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "Encoder-Decoder Architecture\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "- 6 identical layers\n",
    "- each layer\n",
    "  - multi-head self-attention (w/ residual connection)\n",
    "  - position-wise feedforward (w/ residual connection)\n",
    "\n",
    "```python\n",
    "\n",
    "layer_1 = LayerNorm(MultiHeadSelfAttention(x) + x)\n",
    "layer_2 = LayerNorm(PositionWiseFeedforward(x) + x)\n",
    "\n",
    "```\n",
    "\n",
    "#### Decoder\n",
    "\n",
    "- 6 identical layers\n",
    "- each layer\n",
    "  - multi-head self-attention (w/ residual connection)\n",
    "  - multi-head attention (w/ residual connection)\n",
    "  - position-wise feedforward (w/residual connection)\n",
    "\n",
    "```python\n",
    "\n",
    "layer_1 = LayerNorm(MultiHeadAttention(x) + x)\n",
    "layer_2 = LayerNorm(MultiHeadAttention(qk: encoder_output, v: x))\n",
    "layer_3 = LayerNorm(PositionWiseFeedforward(x) + x)\n",
    "\n",
    "```\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled dot-product attention\n",
    "\n",
    "- Attention의 주요 알고리즘으로 Scaled dot-product attention을 사용\n",
    "- dot-product\n",
    "  - attention을 구하는 연산으로 Mat Mul을 사용\n",
    "- scaled\n",
    "  - dimension of key and value. 즉, word embedding vector의 dimension dk\n",
    "  - 1/sqrt(dk)로 attension을 scaling\n",
    "\n",
    "\n",
    "```python\n",
    "ScaledDotProduct(Q,K,V) = Softmax(Q@K.T / sqrt(dk))@V\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ScaledDotProductAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, device=None, dtype: torch.dtype=torch.float, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.dk = torch.sqrt(torch.scalar_tensor(d_model, device=device, dtype=dtype))\n",
    "    \n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        # input should have (B,N,d_model)\n",
    "        # q (b,1,d_model) , k (b,n,d_model)\n",
    "        # qk = (b,1,n)\n",
    "        scaled_qk = q@torch.transpose(k, 2, 1) / self.dk\n",
    "        if mask is not None:\n",
    "            scaled_qk = scaled_qk * mask\n",
    "        attention_weights = torch.softmax(scaled_qk, dim=-1)\n",
    "        return  attention_weights @  v\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head attention\n",
    "\n",
    "- d_model 즉, Query, Key ,Value의 word embedding vector를 다수의 sub vector로 나누어서 각 sub vector를 입력으로 하는 scaled dot-product attention 다수를 조합하여 하나의 Attention block을 구성함.\n",
    "\n",
    "```python\n",
    "MultiHeadAttention(Q,K,V,n_heads) = concat(*[ScaledDotProduct(linear(qi),linear(ki),linear(vi)) for qi,ki,vi in zip(Q.split(n_heads), K.split(n_heads), V.split(n_heads))])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, n_head, device=None, dtype: torch.dtype=torch.float, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.n_head = n_head\n",
    "        self.depth = d_model // n_head\n",
    "\n",
    "        self.q_linear = torch.nn.Linear(d_model, d_model, device=device, dtype=dtype)\n",
    "        self.k_linear = torch.nn.Linear(d_model, d_model, device=device, dtype=dtype)\n",
    "        self.v_linear = torch.nn.Linear(d_model, d_model, device=device, dtype=dtype)\n",
    "\n",
    "        self.attns = torch.nn.ModuleList([ScaledDotProductAttention(d_model=self.depth, device=device, dtype=dtype) for _ in range(n_head)])\n",
    "        self.output_linear = torch.nn.Linear(d_model, d_model, device=device, dtype=dtype)\n",
    "                \n",
    "        \n",
    "    def forward(self, input: torch.Tensor, mask:torch.Tensor=None) -> torch.Tensor:\n",
    "        if len(input.shape) == 2:\n",
    "            input = input.unsqueeze(0)\n",
    "        if len(input.shape) != 3:\n",
    "            raise ValueError(f'unsupported tensor shape: {input.shape}, should be form of (B,N,d)')\n",
    "        \n",
    "        b,n,d = input.shape\n",
    "        q = self.q_linear.forward(input).view((b, n, self.n_head, -1))\n",
    "        k = self.k_linear.forward(input).view((b, n, self.n_head, -1))\n",
    "        v = self.v_linear.forward(input).view((b, n, self.n_head, -1))\n",
    "        attn_output = torch.concat([self.attns[i].forward(q[:,:,i,:].view((b,n,self.depth)), \n",
    "                                            k[:,:,i,:].view((b,n, self.depth)), \n",
    "                                            v[:,:,i,:].view((b,n, self.depth)), mask) for i in range(self.n_head)],dim=-1)\n",
    "        return self.output_linear.forward(attn_output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fritzprix/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "test = torch.rand((1, 1024, 512), device=device, dtype=torch.float16)\n",
    "\n",
    "attention = MultiHeadAttention(512, 8, device=device, dtype=torch.float16)\n",
    "attention = torch.compile(attention)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 512])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = attention.forward(test)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position-wise feedforward\n",
    "\n",
    "- Linear - Relu - Linear\n",
    "\n",
    "\n",
    "## Transformer Layer\n",
    "\n",
    "- GPT논문의 Transformer layer는 Attention is all you need의 transformer는 Encoder 유사한 구조로 residual connection을 갖는 Multi-head attention과 position wise feedforward의 2개의 sublayer로 구성되어 있고 각 sublayer의 출력에 layer norm이 추가되는 형태\n",
    "- 첫 GPT 논문에서는 이러한 transformer block 12개를 쌓아 model을 구성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class PositionWiseFeedforward(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model:int, device, dtype: torch.dtype=torch.float, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.pwff = torch.nn.Sequential(torch.nn.Linear(d_model, d_model * 4, device=device,dtype=dtype), \n",
    "                                            torch.nn.GELU(), \n",
    "                                            torch.nn.Linear(4* d_model, d_model, device=device, dtype=dtype))\n",
    "        \n",
    "    def forward(self, input: torch.Tensor)-> torch.Tensor:\n",
    "        return self.pwff.forward(input)\n",
    "\n",
    "\n",
    "class Transformer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_head, d_model, device, dtype:torch.dtype=torch.float, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.mha = MultiHeadAttention(d_model=d_model, n_head=n_head, device=device, dtype=dtype)\n",
    "        self.mha_lnorm = torch.nn.LayerNorm(d_model, device=device,dtype=dtype)\n",
    "        self.pw_ff = PositionWiseFeedforward(d_model=d_model, device=device, dtype=dtype)\n",
    "        self.out_lnorm = torch.nn.LayerNorm(d_model, device=device, dtype=dtype)\n",
    "\n",
    "    def forward(self, input:torch.Tensor) -> torch.Tensor:\n",
    "        mha_output = self.mha_lnorm(input + self.mha.forward(input))\n",
    "        return self.out_lnorm(mha_output + self.pw_ff(mha_output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "transformer = Transformer(8, 512, device=device, dtype=torch.float16)\n",
    "output = transformer.forward(test)\n",
    "output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

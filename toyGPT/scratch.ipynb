{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# paper wrapup - [Attention is All you need](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "## Problem\n",
    "\n",
    "### LSTM, GRU don't scale well\n",
    "\n",
    "- Recurrent\n",
    "  - 기존의 Sequence Modeling을 위해 사용되던 LSTM, GRU는 재귀적 특성(Sequence)으로 인해 Training 병렬화에 제약을 지님\n",
    "  - 여기서 재귀적 특성이란 LSTM 등은 n번째 output을 계산하기 위해서는 n-1번째 계산의 결과물인 hidden state를 필요로하는 것이다.\n",
    "    ```\n",
    "    Y0, H0 = RNN(X0)\n",
    "    Y1, H1 = RNN(X1, H0)\n",
    "    Y2, H2 = RNN(X2, H1) = RNN(X2, RNN(X1, RNN(X0)[1])[1])\n",
    "\n",
    "    ```\n",
    "  - 위 간략한 코드형 표현에서 보듯, 2번째 입력의 결과인 Y2는 X0까지 RNN 함수를 3번 계산해야 하며\n",
    "  - BPTT (Back Propagation Through Time): Back propagation을 위해서는 이 함수의 계산뿐 아니라 중간에 생성된 모든 상태값들을 메모리에 저장하고 있어야 한다.\n",
    "  - 그리고 이는 RNN의 기반 Network의 Depth가 증가하면서 기하급수적으로 연산 복잡도가 증가하게된다.\n",
    "  - Seq: N 그리고 Depth: D일 때 O(N x D)의 연산 복잡도를 갖게됨\n",
    "  - Training에 있어서 BPTT로 인한 Memory 복잡도는 Sequence의 의존도 모델의 길이가 길어질 수록, 그리고 Network이 Deep해질 수록 기하급수적으로 증가하게된다.\n",
    "  \n",
    "    ![unroll_rnn](./assets/unroll_rnn.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그리고 특히 Parallelization의 한계를 극복하기 위해서 Convnet 기반의 여러 방식들(ByteNet, ConvS2S)이 제안되었으나 이들은 Sequence 상의 임의의 2지점의 관계를 모델링하는데 있어 상대적으로 높은 연산 복잡도를 가지고 있어서 Sequence상의 멀리 떨어진 요소간의 특징을 학습시키는데 제약을 가지고 있었다.\n",
    "  - ConvS2S는 O(n)\n",
    "  - ByteNet는 O(logN)\n",
    "- 즉, long distance의 의존성을 적절하게 다룰 수 있으면서 동시에 병렬연산에 유리한 방법의 필요\n",
    "\n",
    "### Transformers\n",
    "\n",
    "- Transformer는 두 지점의 거리 N에 대해서 O(1)의 복잡도를 제공. 즉, Sequence의 두 위치간의 연관성을 계산함에 있어서 그 사이의 간격에 따른 연산 및 메모리 증가가 그만큼 적다는 의미\n",
    "- Self-Attention은 Reading Comprehension, Semantic Representation, 등에 뛰어난 성능을 보였음\n",
    "- Transfomer는 RNN이나 CNN 등을 사용하지 않고 순수하게 Self-Attention만을 사용한 최초의 사례\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "Encoder-Decoder Architecture\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "- 6 identical layers\n",
    "- each layer\n",
    "  - multi-head self-attention (w/ residual connection)\n",
    "  - position-wise feedforward (w/ residual connection)\n",
    "\n",
    "```python\n",
    "\n",
    "layer_1 = LayerNorm(MultiHeadSelfAttention(x) + x)\n",
    "layer_2 = LayerNorm(PositionWiseFeedforward(x) + x)\n",
    "\n",
    "```\n",
    "\n",
    "#### Decoder\n",
    "\n",
    "- 6 identical layers\n",
    "- each layer\n",
    "  - multi-head self-attention (w/ residual connection)\n",
    "  - multi-head attention (w/ residual connection)\n",
    "  - position-wise feedforward (w/residual connection)\n",
    "\n",
    "```python\n",
    "\n",
    "layer_1 = LayerNorm(MultiHeadAttention(x) + x)\n",
    "layer_2 = LayerNorm(MultiHeadAttention(qk: encoder_output, v: x))\n",
    "layer_3 = LayerNorm(PositionWiseFeedforward(x) + x)\n",
    "\n",
    "```\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled dot-product attention\n",
    "\n",
    "- Attention의 주요 알고리즘으로 Scaled dot-product attention을 사용\n",
    "- dot-product\n",
    "  - attention을 구하는 연산으로 MatMul을 사용\n",
    "- scaled\n",
    "  - dimension of key and value. 즉, word embedding vector의 dimension dk\n",
    "  - 1/sqrt(dk)로 attension weight을 scaling\n",
    "\n",
    "\n",
    "```python\n",
    "ScaledDotProduct(Q,K,V) = Softmax(Q@K.T / sqrt(dk))@V\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "class ScaledDotProductAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    \n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        # input should have (B,N,d_model) where B is batch size, N is length of sequence, d_model is the dimension of word embedding vector\n",
    "        # q (B,N,d_model) , k (B,N,d_model)\n",
    "        # qk = (B,N,N)\n",
    "        d_k = k.size(2) # d_model\n",
    "        scaled_qk: torch.Tensor = q@torch.transpose(k, 2, 1) * (1 / math.sqrt(d_k))\n",
    "        # mask should be tensor of bool (B,N,N)\n",
    "        if mask is not None:\n",
    "            scaled_qk = scaled_qk.masked_fill(mask.bitwise_not(), float('-inf'))\n",
    "        attention_weights = torch.softmax(scaled_qk, dim=-1)\n",
    "        return  attention_weights @  v\n",
    "        \n",
    "\n",
    "\n",
    "## From the outside of the ScaledDotProductAttention\n",
    "    \n",
    "batch_size = 4\n",
    "seq_length = 8\n",
    "d_model = 32\n",
    "\n",
    "inputs = torch.rand((batch_size, seq_length, d_model))\n",
    "attention_input_projection = torch.nn.Linear(d_model, 3 * d_model)\n",
    "single_attention_head = ScaledDotProductAttention()\n",
    "\n",
    "qkv_bundle:torch.Tensor = attention_input_projection(inputs) # shape of (batch_size, seq_length, d_model) so we have to split this tensor into Q,K,V\n",
    "q,k,v = qkv_bundle.split(d_model, -1) # now we have q,k,v\n",
    "# now we assume that we need causal mask for training here\n",
    "# causal mask is sqaure matrix of (seq_length, seq_length) and all the values lower diagonal elements are `True` because we'll use masked_fill later\n",
    "\n",
    "mask = torch.tril(torch.ones(seq_length, seq_length, dtype=torch.bool)).unsqueeze(0) # we have to unsqueeze here, so it can be broadcasted\n",
    "\n",
    "fake_masked_attention_weights = torch.rand((batch_size, seq_length, seq_length)).masked_fill(mask.bitwise_not(), float('-inf'))\n",
    "print(f\"Shape of Mask : \\n{fake_masked_attention_weights[0]}\")\n",
    "output_of_attention_head = single_attention_head(q,k,v,mask) # the output of attention head is (batch_size, seq_length, d_model) it's same to the input\n",
    "print(f\"Shape of Attention Output:\\n{output_of_attention_head.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head attention\n",
    "\n",
    "- d_model 즉, Query, Key ,Value의 word embedding vector를 다수의 sub vector로 나누어서 각 sub vector를 입력으로 하는 scaled dot-product attention 다수를 조합하여 하나의 Attention block을 구성함.\n",
    "\n",
    "```python\n",
    "MultiHeadAttention(Q,K,V,n_heads) = concat(*[ScaledDotProduct(linear(qi),linear(ki),linear(vi)) for qi,ki,vi in zip(Q.split(n_heads), K.split(n_heads), V.split(n_heads))])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model:int, n_head:int, device=None, dtype: torch.dtype=torch.float32, dropout:float=0.2, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.n_head = n_head\n",
    "        self.d_head = d_model // n_head\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.qkv_proj = torch.nn.Sequential(\n",
    "            torch.nn.Linear(d_model, 3 * d_model, device=device, dtype=dtype), \n",
    "            torch.nn.Dropout(dropout))\n",
    "\n",
    "        self.attns = torch.nn.ModuleList([ScaledDotProductAttention() for _ in range(n_head)])\n",
    "        \n",
    "        self.output_linear = torch.nn.Linear(d_model, d_model, device=device, dtype=dtype)\n",
    "        self.out_drop = torch.nn.Dropout(dropout)\n",
    "\n",
    "        \n",
    "    def forward(self, input: torch.Tensor, mask: torch.Tensor=None) -> torch.Tensor:\n",
    "        if len(input.shape) == 2:\n",
    "            input = input.unsqueeze(0)\n",
    "        if len(input.shape) != 3:\n",
    "            raise ValueError(f'unsupported tensor shape: {input.shape}, should be form of (B,N,d)')\n",
    "        \n",
    "        batch_size,seq_n, d_model = input.shape\n",
    "        qkv_bundle: torch.Tensor = self.qkv_proj(input)\n",
    "        q,k,v = qkv_bundle.split(d_model,-1)\n",
    "        \n",
    "        q = q.view((batch_size, seq_n, -1, self.d_head)).transpose(1, 2)\n",
    "        k = k.view((batch_size, seq_n, -1, self.d_head)).transpose(1, 2)\n",
    "        v = v.view((batch_size, seq_n, -1, self.d_head)).transpose(1, 2)\n",
    "        # now Q,K,V have shape of (batch_size, seq_n, n_head, d_head)\n",
    "\n",
    "\n",
    "        attn_output = torch.concat([self.attns[i].forward(q[:,i,:,:].view((batch_size,seq_n,self.d_head)), \n",
    "                                            k[:,i,:,:].view((batch_size,seq_n, self.d_head)), \n",
    "                                            v[:,i,:,:].view((batch_size,seq_n, self.d_head)),mask=mask) for i in range(self.n_head)],dim=-1)\n",
    "        \n",
    "        return self.out_drop(self.output_linear(attn_output))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## From the outside of the MultiHeadAttention\n",
    "    \n",
    "batch_size = 4\n",
    "seq_length = 8\n",
    "d_model = 32\n",
    "n_head = 4\n",
    "\n",
    "inputs = torch.rand((batch_size, seq_length, d_model))\n",
    "attention_input_projection = torch.nn.Linear(d_model, 3 * d_model)\n",
    "multi_head_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "\n",
    "mask = torch.tril(torch.ones(seq_length, seq_length, dtype=torch.bool)).unsqueeze(0) # we have to unsqueeze here, so it can be broadcasted\n",
    "\n",
    "fake_masked_attention_weights = torch.rand((batch_size, seq_length, seq_length)).masked_fill(mask.bitwise_not(), float('-inf'))\n",
    "print(f\"Shape of Mask : \\n{fake_masked_attention_weights[0]}\")\n",
    "output_of_attention_head = multi_head_attention(inputs,mask) # the output of attention head is (batch_size, seq_length, d_model) it's same to the input\n",
    "print(f\"Shape of Attention Output:\\n{output_of_attention_head.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Masking` in Masked Attention\n",
    "\n",
    "- Masking은 Scaled(Q@K.T)의 결과인 Attention Score에서 QnKm (where m > n)에 있는 값들을 -inf으로 변환한다.\n",
    "- 이는 GPT와 같은 Autoregressive 모델의 주요한 목적, 즉, Sequence에서 한쪽의 정보를 기반으로 그 다음에 올 정보를 추정해야하는 모델에서 \n",
    "- 추정의 정답 즉, Ground Truth에 대한 정보가 Training에 포함되는 것을 막기 위함이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example sequence including special tokens\n",
    "sequence = [\"<bos>\", \"hello\", \"there\", \"I'm\", \"david\", \"<eos>\"]\n",
    "seq_length = len(sequence)\n",
    "\n",
    "# Creating a mask with large negative numbers for visualization\n",
    "filled_mask = np.triu(np.ones((seq_length, seq_length)), k=1) * -10\n",
    "\n",
    "# Plotting the filled mask\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(filled_mask, cmap='viridis')\n",
    "plt.colorbar(label='Mask Value')\n",
    "plt.title('Filled Attention Mask Visualization for \"hello there I\\'m david\"')\n",
    "plt.xlabel('Key Tokens')\n",
    "plt.ylabel('Query Tokens')\n",
    "plt.xticks(ticks=range(seq_length), labels=sequence)\n",
    "plt.yticks(ticks=range(seq_length), labels=sequence)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position-wise feedforward\n",
    "\n",
    "- Linear - Relu - Linear\n",
    "- dimension of hidden is 4 times of input\n",
    "- but we'll use GELU instead of ReLU following the GPT-2 paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class PositionWiseFeedforward(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, d_model:int, device, dtype: torch.dtype=torch.float32, dropout=0.2, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.pwff = torch.nn.Sequential(torch.nn.Linear(d_model, d_model * 4, device=device,dtype=dtype), \n",
    "                                            torch.nn.GELU(), \n",
    "                                            torch.nn.Linear(4* d_model, d_model, device=device, dtype=dtype), \n",
    "                                            torch.nn.Dropout(dropout))\n",
    "        \n",
    "    def forward(self, input: torch.Tensor)-> torch.Tensor:\n",
    "        return self.pwff.forward(input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Transformer Layer\n",
    "\n",
    "- GPT논문의 Transformer layer는 Attention is all you need의 transformer는 Encoder 유사한 구조로 residual connection을 갖는 Multi-head attention과 position wise feedforward의 2개의 sublayer로 구성되어 있고 각 sublayer의 출력에 layer norm이 추가되는 형태\n",
    "- Encoder와 Cross Attention의 sublayer를 없애고 MHA(Multi-Head Attention Layer) + PWFF (Poisition-Wise Feedforward)만으로 Transformer block을 구성\n",
    "- 첫 GPT 논문에서는 이러한 transformer block 12개를 쌓아 올린 형태의 Network\n",
    "\n",
    "### Normalization\n",
    "\n",
    "- AAYN의 구조에서는 Add -> LayerNorm의 순서 residual 연결을 sublayer의 입력과 병합하였다. 이를 PostNorm이라고 한다.\n",
    "  ```\n",
    "  output = LayerNorm(input + sublayer(input))\n",
    "  ```\n",
    "- GPT3에서에서는 PreNorm 즉, sublayer의 계산보다 먼저 layernorm을 계산하고 sublayer의 output에 직접 residual connection을 적용하는 방식을 적용\n",
    "  ```\n",
    "  output = input + sublayer(LayerNorm(input))\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple\n",
    "\n",
    "class Transformer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_head, d_model, device, dtype:torch.dtype=torch.float32,dropout:float=0.2, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.input_norm = torch.nn.LayerNorm(d_model, device=device, dtype=dtype)\n",
    "        self.mha = MultiHeadAttention(d_model=d_model, n_head=n_head, device=device, dtype=dtype, dropout=dropout)\n",
    "        self.mha_lnorm = torch.nn.LayerNorm(d_model, device=device,dtype=dtype)\n",
    "        self.pw_ff = PositionWiseFeedforward(d_model=d_model, device=device, dtype=dtype, dropout=dropout)\n",
    "\n",
    "    def forward(self, data:Tuple[torch.Tensor]) -> torch.Tensor:\n",
    "        # Pre-LayerNormalization from GPT-3, (note: Post-LayerNormalization is used for GPT-2 and original paper)\n",
    "        input, attention_mask = data\n",
    "\n",
    "        norm_input = self.input_norm.forward(input)\n",
    "        mha_output = input + self.mha.forward(norm_input, attention_mask)\n",
    "        norm_mha_output = self.mha_lnorm(mha_output)\n",
    "        return (mha_output + self.pw_ff.forward(norm_mha_output), attention_mask)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') # use CUDA whenever it's available\n",
    "\n",
    "batch_size = 8\n",
    "d_model = 1024\n",
    "seq_length = 512\n",
    "n_head = 8\n",
    "\n",
    "inputs = (torch.rand((batch_size, seq_length, d_model), device=device), torch.tril(torch.ones((1, seq_length, seq_length), dtype=torch.bool, device=device)))\n",
    "transformer = Transformer(n_head=n_head, d_model=d_model, device=device, dtype=torch.float)\n",
    "\n",
    "output = transformer.forward(inputs)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding & Tokenization\n",
    "\n",
    "- Text를 적절한 Numerical Representation으로 변형을 하기 위해서는 크게 2가지의 처리 단계를 필요로 하는데 이는 Tokenization과 Embedding이다.\n",
    "\n",
    "\n",
    "### Tokenization\n",
    "- Text를 최소 단위로 쪼개어 각 의미 단위에 고유한 ID를 부여하는 것.\n",
    "- 최소 단위로 쪼개는 방식에 따라 Word Level Tokenization, Character Level Tokenization 부터 Subword Tokenization 등 다양한 방법이 있다. \n",
    "- 단, 신규 어휘의 확장(합성어 등)에 유연하게 대응할 수 있는 장점을 지닌 Subword 방식을 많이 쓰고 있으며 BPE라는 방식이 유명하다.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "vocab_size = 500\n",
    "dense_rep = 50\n",
    "batch = 8\n",
    "seq = 64\n",
    "\n",
    "input = torch.ones((batch, seq), dtype=torch.int) ## test input\n",
    "\n",
    "embedding = torch.nn.Embedding(vocab_size, dense_rep) ## convert the token_id to dense vector\n",
    "\n",
    "output = embedding.forward(input=input)\n",
    "\n",
    "output.shape # would be (8,64,50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "- Embedding은 token을 다차원의 vector로 변환시키는 기능을 한다.\n",
    "- Tokenizer에 의해 고유하게 변환될 수 있는 ID의 다양성, 예를 들어 단어를 기준으로 한다면 어휘의 종류를 `vocab_size` 그리고 word vector의 dimension을 n_embed라고 하면 Embedding의 parameter는 ```vocab_size * n_embed```개가 된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "vocab_size = 50000 # total number of possible ID for input token\n",
    "n_embed = 1024 # dense vector representation for each token\n",
    "\n",
    "embedding = torch.nn.Parameter(data=torch.zeros((vocab_size, n_embed)), requires_grad=True)\n",
    "\n",
    "input_ids = [1,2,3,4]\n",
    "\n",
    "w_vectors = torch.stack([embedding[id] for id in input_ids])\n",
    "print(f\"Shape of Word Vectors: \\n{w_vectors.shape}\")\n",
    "\n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding - Absolute Position Encoding from AAYN paper\n",
    "- Embedding을 통하여 언어를 Deep Learning에 활용할 수 있는 Dense Vector로 변환할 수 있지만 이렇게 생성된 Vector는 해당 token의 위치를 고려하지 않는다.\n",
    "- 따라서 각 token의 상대적 배열 순서에 대한 정보를 추가하기 위한 수단이 필요하며 이것이 바로 Positional Encoding이다.\n",
    "- AAYN 논문에서는 sinusodial function을 이용하여 이러한 Positional Encoding을 구현하였다.\n",
    "  - 단순히 position에 따라 embedding vector에 동일한 값을 적용하는 방식이 아닌\n",
    "  - embedding vector의 subspace에따라서 sinusoid의 period가 다르게 적용되도록 embedding dimension을 position embedding의 변수로 추가\n",
    "  - 이로써 sinusoidal function의 반복적 특징에 의한 값의 중복을 없애고\n",
    "  - suquence 내의 모든 값에 대해서 고유한 위치 정보를 할당할 수 있음\n",
    "- 이러한 경우 PE는 learnable parameter가 아닌 position과 word vector dimension에 따라 정해진 상수임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def positional_embedding(d_model, max_length, dtype=None, device=None):\n",
    "    div_even = torch.pow(10000, torch.arange(0, d_model // 2, dtype=dtype, device=device) * 2 / d_model)\n",
    "    div_odd = torch.pow(10000, torch.arange(0, d_model // 2, dtype=dtype, device=device) * 2 / d_model)\n",
    "    pos = torch.arange(0, max_length, device=device, dtype=dtype).unsqueeze(-1)\n",
    "    pe = torch.zeros((max_length, d_model))\n",
    "    pe[:,0::2] = torch.sin(pos / div_even)\n",
    "    pe[:,1::2] = torch.cos(pos / div_odd)\n",
    "    return pe.requires_grad_(False)\n",
    "\n",
    "# Parameters\n",
    "d_model = 20\n",
    "max_length = 100\n",
    "\n",
    "# Generate positional embeddings\n",
    "pe = positional_embedding(d_model, max_length)\n",
    "\n",
    "# Plot each dimension separately\n",
    "for i in range(int(d_model / 2)):\n",
    "    plt.plot(pe[:, i].numpy(), label=f'Dimension {i}')\n",
    "\n",
    "plt.title('Positional Encoding Waveforms for Each Dimension')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Encoding Value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding - Learnable Positional Encoding\n",
    "- Karpathy의 nanoGPT의 경우 `torch.nn.Embedding`을 그대로 positional embedding에 사용하였다.\n",
    "- 실제 AAYN 논문에도 제안된 sinusoidal method와 성능측면에서 큰 차이가 없었음을 언급하고 있다.\n",
    "- 하지만 learnable embedding의 경우 학습에 의해서 생성되는 것이기 때문에 longer sequence에 대한 대응에 제약이 상대적으로 더 클것이고\n",
    "- 일정한 규칙을 갖는 sinudoidal method가 이러한 측면에서 더 우수한 일반화 성능을 보일 것이라는 이유에서 sinudoidal 방법을 채택하였다고 언급하고 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "max_length = 512\n",
    "d_model = 768\n",
    "batch_size = 4\n",
    "seq_n = 32\n",
    "\n",
    "wpe = torch.nn.Embedding(max_length, d_model)\n",
    "inputs = torch.rand((batch_size, seq_n, d_model))\n",
    "\n",
    "pe = wpe(torch.arange(0, inputs.size(1))).unsqueeze(0)\n",
    "embedding = inputs + pe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToyGPT\n",
    "\n",
    "### Structure\n",
    "\n",
    "- 위에 정의된 각 module을 조합하여 GPT 모델을 아래와 같이 구성\n",
    "- Positional Embedding은 learnable absolute position encoding 방식을 사용 (Karpathy의 nanoGPT와 동일)\n",
    "- Sequential로 묶여 있는 연속된 Transformer layers와 이후 dense vector에서 token logits output을 얻기 위한 linear layer로 구성\n",
    "\n",
    "#### Weight Tying\n",
    "\n",
    "- Input embedding layer(token_ids => dense word vector)와 output embedding layer (dense word vector => token_ids)의 weight을 share (tying)함으로써 얻을 수 있는 이득은\n",
    "  - Parameter의 효율적 사용과 동시에 전반적인 성능의 향상\n",
    "  - inference 시 latency 감소\n",
    "\n",
    "### training parameter\n",
    "\n",
    "- AdamW optimizer를 사용\n",
    "- learning rate\n",
    "  -  GPT-2 paper의 설정을 일부 사용\n",
    "  -  lr: 2.5e-4\n",
    "  -  2000 steps of soft warmup\n",
    "  -  weight initialization N[0, 0.02]\n",
    "  -  dropout: 0.1 (attention, residual, embedding)\n",
    "  -  weight decay: 0.1 (which is from GPT-3 paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Tuple, Dict\n",
    "from lightning.pytorch.utilities.types import STEP_OUTPUT, OptimizerLRScheduler\n",
    "import torch\n",
    "import lightning as L\n",
    "\n",
    "\n",
    "class ToyGPT(L.LightningModule):\n",
    "\n",
    "    def __init__(self, \n",
    "                 vocab_size:int, \n",
    "                 block_size:int,\n",
    "                 n_embed:int, n_head:int, n_layer:int, pad_id:int=None,  device=None, \n",
    "                 dtype:torch.dtype=torch.float32, dropout:float=0.1, lr=2.5e-4, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01, name: str='toygpt', *args, **kwargs) -> None:\n",
    "        \n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.save_hyperparameters(ignore=['dtype', 'device'])\n",
    "        self.name = name\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.eps = eps\n",
    "        self.decay = weight_decay\n",
    "        self.output_linear = torch.nn.Linear(n_embed, vocab_size, device=device, dtype=dtype)\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, n_embed, padding_idx=pad_id) \n",
    "        # I referred to the nanoGPT of Karpathy about weight tying. embedding layer that convert token IDs to dense vector and the linear layer followed by softmax at the output of language model has directly reversed functionality to each other.\n",
    "        # benefit of weight tying\n",
    "        self.embedding.weight = self.output_linear.weight \n",
    "\n",
    "\n",
    "        self.pos_embedding = positional_embedding(d_model=n_embed, max_length=block_size, device=device, dtype=dtype).unsqueeze(0)\n",
    "        self.embedding_dropout = torch.nn.Dropout(dropout)\n",
    "        self.transformers = torch.nn.Sequential(*[Transformer(n_head=n_head, d_model=n_embed, device=device, dtype=dtype, dropout=dropout) for _ in range(n_layer)])\n",
    "        self.loss = torch.nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, torch.nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def configure_optimizers(self) -> OptimizerLRScheduler:\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, betas=self.betas, eps=self.eps, weight_decay=self.decay)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer=optimizer,schedulers=[\n",
    "            torch.optim.lr_scheduler.LinearLR(optimizer=optimizer, start_factor=self.eps, total_iters=2000, end_factor=1),\n",
    "            torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=4000, eta_min=(self.lr / 10))\n",
    "        ],milestones=[2000])\n",
    "        lr_scheduler_config = {\n",
    "            \"scheduler\": lr_scheduler,\n",
    "            \"interval\": \"step\",\n",
    "            \"frequency\": 1,\n",
    "            \"name\": \"CosineWithWarmUp\",\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}\n",
    "\n",
    "    def forward(self, input: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        # X should have shape of (B,N)\n",
    "        X: torch.Tensor = input[\"input_ids\"]\n",
    "        attention_mask: torch.Tensor = input[\"attention_mask\"]\n",
    "        if len(X.shape) == 1:\n",
    "            X = X.unsqueeze(0)\n",
    "        N = X.size(1)\n",
    "        \n",
    "        X_wemb = self.embedding(X) + self.pos_embedding[:,:N,:] # word embedding + postion embedding\n",
    "        hs, _ = self.transformers.forward((X_wemb, attention_mask.bool()))\n",
    "        return torch.softmax(self.output_linear.forward(hs[:,-1,:]), -1)\n",
    "\n",
    "\n",
    "    def training_step(self, data: Tuple[torch.Tensor], batch_index:Any, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\n",
    "        \n",
    "        X = data[\"input_ids\"]\n",
    "\n",
    "        input:torch.Tensor = X[:,:-1]\n",
    "        target:torch.Tensor = X[:,1:].long()\n",
    "        B,n = input.shape\n",
    "\n",
    "        attention_mask:torch.Tensor = torch.tril(torch.ones((n,n), device=input.device)).unsqueeze(0).expand((B,n,n)).bool()\n",
    "        \n",
    "\n",
    "        # X_wemb = self.embedding(input) + self.pos_embedding(torch.arange(0, input.shape[-1],device=input.device, dtype=torch.long)) # word embedding + postion embedding\n",
    "        X_wemb = self.embedding(input) + self.pos_embedding[:,:n,:] # word embedding + postion embedding\n",
    "        hidden_output, _ = self.transformers.forward((self.embedding_dropout(X_wemb), attention_mask))\n",
    "        logits = self.output_linear.forward(hidden_output)\n",
    "        # the sequencess of batch are now totally flatten into (B * n, logits), so we have to divide the loss by batch_size\n",
    "        loss = self.loss(logits.view(-1, logits.size(-1)), target.reshape(-1))\n",
    "        if batch_index % 10 == 0:\n",
    "            lr = self.trainer.optimizers[0].param_groups[0]['lr']\n",
    "            # log train loss not too much frequently\n",
    "            self.log(\"train_loss\", loss)\n",
    "            self.log(\"lr\", lr)\n",
    "\n",
    "        return {\"batch_index\": batch_index, \"loss\":loss}\n",
    "    \n",
    "\n",
    "    def validation_step(self, data: Tuple[torch.Tensor], batch_index,*args: Any, **kwargs: Any) -> STEP_OUTPUT:\n",
    "        \n",
    "        X = data[\"input_ids\"]\n",
    "\n",
    "        input:torch.Tensor = X[:,:-1]\n",
    "        target:torch.Tensor = X[:,1:].long()\n",
    "        B,n = input.shape\n",
    "\n",
    "        attention_mask:torch.Tensor = torch.tril(torch.ones((n,n), device=input.device)).unsqueeze(0).expand((B,n,n)).bool()\n",
    "\n",
    "        X_wemb = self.embedding(input) + self.pos_embedding[:,:n,:] # word embedding + postion embedding\n",
    "        hidden_output, _ = self.transformers.forward((self.embedding_dropout(X_wemb), attention_mask))\n",
    "\n",
    "        logits = self.output_linear.forward(hidden_output)\n",
    "        # the sequencess of batch are now totally flatten into (B * n, logits), so we have to divide the loss by batch_size\n",
    "        loss = self.loss(logits.view(-1, logits.size(-1)), target.reshape(-1))\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return {\"batch_index\": batch_index, \"val_loss\":loss}\n",
    "    \n",
    "    \n",
    "    def test_step(self, data: Tuple[torch.Tensor], batch_index, *args: Any, **kwargs: Any) -> STEP_OUTPUT:\n",
    "       \n",
    "        X = data[\"input_ids\"]\n",
    "\n",
    "        input:torch.Tensor = X[:,:-1]\n",
    "        target:torch.Tensor = X[:,1:].long()\n",
    "        B,n = input.shape\n",
    "\n",
    "        attention_mask:torch.Tensor = torch.tril(torch.ones((n,n), device=input.device)).unsqueeze(0).expand((B,n,n)).bool()\n",
    "\n",
    "        X_wemb = self.embedding(input) + self.pos_embedding[:,:n,:] # word embedding + postion embedding\n",
    "        hidden_output, _ = self.transformers.forward((self.embedding_dropout(X_wemb), attention_mask))\n",
    "\n",
    "        logits = self.output_linear.forward(hidden_output)\n",
    "        # the sequencess of batch are now totally flatten into (B * n, logits), so we have to divide the loss by batch_size\n",
    "        loss = self.loss(logits.view(-1, logits.size(-1)), target.reshape(-1))\n",
    "        self.log(\"test_loss\", loss)\n",
    "\n",
    "        return {\"batch_index\": batch_index, \"val_loss\":loss}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "- Pre-trained GPT2 tokenizer will be used\n",
    "- Wikisource.en will be used as training data\n",
    "- Special Tokens\n",
    "  - pad_token: `<pad>`\n",
    "  - bos_token: `<|startoftext|>`\n",
    "  - eos_token: `<|endoftext|>`\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, GPT2Tokenizer\n",
    "\n",
    "def get_tokenizer() -> PreTrainedTokenizer:\n",
    "    \n",
    "    tokenizer: PreTrainedTokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"<pad>\", \"bos_token\":\"<|startoftext|>\", \"eos_token\":\"<|endoftext|>\"}) # special \n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer()\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "with open('config.json') as fp:\n",
    "    configs = json.load(fp)\n",
    "\n",
    "for config in configs:\n",
    "    model = ToyGPT(vocab_size=len(tokenizer), pad_id=tokenizer.pad_token_id, device=device, **config)\n",
    "    total_params = 0\n",
    "    for param in model.parameters():\n",
    "        total_params += param.numel()\n",
    "    print(f'{model.name} : params {total_params / 1e6}M')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_count = 0\n",
    "for p in model.parameters():\n",
    "    param_count += p.numel()\n",
    "\n",
    "print(f\"parameters : {param_count/1e6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameter\n",
    "\n",
    "- GPT2Tokenizer의 Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "- \"wikimedia/wikisource\", \"20231201.en\"\n",
    "- HuggingFace의 Datasets와 Unstructured.io를 이용 Data의 download, pre-processing을 구현\n",
    "- Lightening framework과 연동을 위해 LighteningDataModule을 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Dict\n",
    "\n",
    "import lightning as L\n",
    "from datasets import Dataset, load_from_disk, load_dataset\n",
    "from lightning.pytorch.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS\n",
    "from transformers import PreTrainedTokenizer\n",
    "from torch.utils import data\n",
    "from unstructured.cleaners.core import (\n",
    "    replace_unicode_quotes, clean, clean_ligatures\n",
    ")\n",
    "import re\n",
    "from unstructured.cleaners.core import clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Pre-processing\n",
    "- Text Cleansing\n",
    "  - extra whitespace 제거\n",
    "  - unicode quote 대체\n",
    "  - ligatures 제거\n",
    "- 이렇게 Cleansing된 Text를 완전한 Sentence 단위로 분절하여 Max Length에 맞도록 전처리하여 Local에 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceChunker:\n",
    "    \"\"\"\n",
    "    A class responsible for chunking text into sentences and tokenizing them\n",
    "    according to a specified maximum length.\n",
    "\n",
    "    Attributes:\n",
    "        tokenizer (PreTrainedTokenizer): A tokenizer from the transformers library\n",
    "                                         used for tokenizing sentences.\n",
    "        max_length (int): The maximum token length for a single chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    def _split_into_sentences(self, text):\n",
    "        \"\"\"\n",
    "        Splits the input text into sentences.\n",
    "\n",
    "        The text is first cleaned to standardize it (removing extra whitespaces, \n",
    "        replacing unicode quotes, and removing ligatures). Then, it is split into \n",
    "        sentences using a regular expression that looks for sentence end markers \n",
    "        (., !, ?) followed by a whitespace.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be split into sentences.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: A list of sentences extracted from the input text.\n",
    "        \"\"\"\n",
    "        # Clean the text and split it into sentences\n",
    "        clean_text = replace_unicode_quotes(clean_ligatures(clean(text, extra_whitespace=True)))\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', clean_text)\n",
    "        return [f'{sentence}' for sentence in sentences]\n",
    "\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, max_length:int) -> None:\n",
    "        print('test v1.0')\n",
    "        \"\"\"\n",
    "        Initializes the SentenceChunker with a tokenizer and a maximum length.\n",
    "\n",
    "        Args:\n",
    "            tokenizer (PreTrainedTokenizer): The tokenizer to be used for tokenization.\n",
    "            max_length (int): The maximum token length for a single chunk.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, batch, *args: Any, **kwds: Any) -> Any:\n",
    "        \"\"\"\n",
    "        Processes a batch of text sequences by first splitting them into sentences,\n",
    "        then encoding each sentence. The sentences are then chunked according to the \n",
    "        maximum length, ensuring no chunk exceeds this limit.\n",
    "\n",
    "        Args:\n",
    "            batch: A batch of text sequences.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, List]: A dictionary with two keys, 'success' and 'failure'.\n",
    "                             'success' contains chunks that are within the max_length,\n",
    "                             'failure' contains chunks that exceed the max_length.\n",
    "        \"\"\"\n",
    "        # Handle single string inputs by wrapping them in a list\n",
    "        if isinstance(batch, str):\n",
    "            batch = [batch]\n",
    "\n",
    "\n",
    "        # Split each sequence in the batch into sentences and encode them\n",
    "        batch_of_chunks = [self._split_into_sentences(seq) for seq in batch]\n",
    "        batch_of_encodings = [self.tokenizer.batch_encode_plus(chunks, return_length=True, add_special_tokens=True) for chunks in batch_of_chunks]\n",
    "\n",
    "        result = {\"success\": [], \"failure\": []}\n",
    "        success_batch_bucket = []\n",
    "        failure_batch_bucket = []\n",
    "\n",
    "        # Iterate over each sequence's encodings and chunk them\n",
    "        for bi, encodings in enumerate(batch_of_encodings):\n",
    "            bucket = []\n",
    "            tokens_total = 0\n",
    "\n",
    "            # Process each sentence in the sequence\n",
    "            for n, token_count in enumerate(encodings[\"length\"]):\n",
    "                token_count += 1 # splitting sequence removes space between two adjacent sequence in the process, so 1 token is accounted\n",
    "                # Handle sentences that exceed the max length\n",
    "                if token_count > self.max_length:\n",
    "                    failure_batch_bucket.append({\"text\":batch_of_chunks[bi][n], \"length\": token_count})\n",
    "                    if len(bucket) > 0:\n",
    "                        success_batch_bucket.append({\"text\":' '.join(bucket), \"length\": tokens_total})\n",
    "                        bucket.clear()\n",
    "                        tokens_total = 0\n",
    "                    continue\n",
    "\n",
    "                # Check if adding the sentence would exceed the max length\n",
    "                if token_count + tokens_total > self.max_length:\n",
    "                    # Current bucket is full, save and reset it\n",
    "                    seq = ' '.join(bucket)\n",
    "                    tokens = self.tokenizer.encode(seq)\n",
    "                    if len(tokens) > self.max_length:\n",
    "                        print(f\"{len(tokens)} vs {self.max_length}\")\n",
    "                        assert False\n",
    "                    success_batch_bucket.append({\"text\":seq, \"length\": tokens_total})\n",
    "                    bucket.clear()\n",
    "                    tokens_total = 0\n",
    "                \n",
    "                # Add the sentence to the current bucket\n",
    "                bucket.append(batch_of_chunks[bi][n])\n",
    "                tokens_total += token_count\n",
    "\n",
    "            # Append the processed batches to the result\n",
    "            result[\"success\"].append([*success_batch_bucket])\n",
    "            result['failure'].append([*failure_batch_bucket])\n",
    "            success_batch_bucket.clear()\n",
    "            failure_batch_bucket.clear()\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Loading\n",
    "- shuffling\n",
    "- batch tokenization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Dict\n",
    "import shutil\n",
    "\n",
    "import random\n",
    "\n",
    "import lightning as L\n",
    "from datasets import Dataset, load_from_disk, load_dataset\n",
    "from lightning.pytorch.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS\n",
    "from transformers import PreTrainedTokenizer\n",
    "from torch.utils import data\n",
    "from unstructured.cleaners.core import (\n",
    "    replace_unicode_quotes, clean, clean_ligatures\n",
    ")\n",
    "import re\n",
    "import os\n",
    "from unstructured.cleaners.core import clean\n",
    "\n",
    "import random\n",
    "\n",
    "class SuccessCaseGenerator:\n",
    "    def __init__(self, datasets: List[Dataset], transform=None) -> None:\n",
    "        self.datasets = datasets\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, *args: Any, **kwds: Any) -> Any:\n",
    "        for ds in self.datasets:\n",
    "            for b in ds[\"success\"]:\n",
    "                for seq in b:\n",
    "                    if self.transform:\n",
    "                        seq = self.transform(seq)\n",
    "                    yield seq\n",
    "\n",
    "def random_indices(total_elements, portion):\n",
    "    # Calculate the number of elements to select\n",
    "    number_to_select = round(total_elements * portion)\n",
    "\n",
    "    # Generate a list of unique indices for selection\n",
    "    selected_indices = random.sample(range(total_elements), number_to_select)\n",
    "\n",
    "    # Calculate the not-selected indices\n",
    "    all_indices = set(range(total_elements))\n",
    "    not_selected_indices = list(all_indices - set(selected_indices))\n",
    "\n",
    "    return selected_indices, not_selected_indices\n",
    "\n",
    "\n",
    "class HuggingFaceCollectionModule(L.LightningDataModule):\n",
    "\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer, \n",
    "                 paths:List[str],\n",
    "                 subsets: List[List[str]],\n",
    "                 max_length:int, \n",
    "                 batch_size:int,\n",
    "                 clear_cache:bool=False,\n",
    "                 train_size:float=0.9,\n",
    "                 resume_pos = 0,\n",
    "                 num_proc=15) -> None:\n",
    "        super().__init__()\n",
    "        self.name = '_'.join(paths)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.paths = paths\n",
    "        self.subsets = subsets\n",
    "        self.max_length = max_length\n",
    "        self.train_size = train_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_proc = num_proc\n",
    "        self.clear_cache = clear_cache\n",
    "        self.resume_pos = resume_pos\n",
    "        self.dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.train_dataset = None\n",
    "        self.local_fdata_cache_path = f'cache/{self.name}/local_dscache'\n",
    "        self.local_tdata_cache_path = f'cache/{self.name}/train_dscache'\n",
    "        self.local_vdata_cache_path = f'cache/{self.name}/val_dscache'\n",
    "        self.local_tokenized_cache_path = f'cache/{self.name}/tokenized'\n",
    "\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "\n",
    "        full_dataset = None\n",
    "\n",
    "        def transform(v:Dict[str, Any]) -> str:\n",
    "            return {\"length\": v[\"length\"], \"text\": f\"<|startoftext|>{v['text']}<|endoftext|>\"}\n",
    "        \n",
    "        if self.clear_cache:\n",
    "            shutil.rmtree(self.local_fdata_cache_path, ignore_errors=True)\n",
    "            shutil.rmtree(self.local_tdata_cache_path, ignore_errors=True)\n",
    "            shutil.rmtree(self.local_vdata_cache_path, ignore_errors=True)\n",
    "        \n",
    "        if not os.path.exists(self.local_fdata_cache_path):\n",
    "\n",
    "            sentence_chunker = SentenceChunker(self.tokenizer, self.max_length - 2)\n",
    "            datasets = [load_dataset(path, subset)['train'].map(lambda b: sentence_chunker(b[\"text\"]), batched=True, num_proc=self.num_proc).flatten()\n",
    "             for i, path in enumerate(self.paths) for subset in self.subsets[i]]\n",
    "                \n",
    "            full_dataset = Dataset.from_generator(SuccessCaseGenerator(datasets, transform=transform))\n",
    "            full_dataset = full_dataset.train_test_split(test_size=(1 - self.train_size), train_size=self.train_size)\n",
    "            full_dataset.save_to_disk(self.local_fdata_cache_path)\n",
    "        else:\n",
    "            print('full data cached locally')\n",
    "        \n",
    "        if not (os.path.exists(self.local_tdata_cache_path) and os.path.exists(self.local_vdata_cache_path)):\n",
    "            if full_dataset is None:\n",
    "                full_dataset = load_from_disk(self.local_fdata_cache_path)\n",
    "            visible_dataset = full_dataset['train'].shuffle()\n",
    "            print(visible_dataset)\n",
    "            val_selection, train_selection = random_indices(len(visible_dataset), (1 - self.train_size))\n",
    "            val_dataset = visible_dataset.select(val_selection)\n",
    "            train_dataset = visible_dataset.select(train_selection)\n",
    "            val_dataset.save_to_disk(self.local_vdata_cache_path)\n",
    "            train_dataset.save_to_disk(self.local_tdata_cache_path)\n",
    "        else:\n",
    "            print('load from local cache')\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        if self.dataset is None:\n",
    "            self.dataset = load_from_disk(self.local_fdata_cache_path)\n",
    "            self.val_dataset = load_from_disk(self.local_vdata_cache_path)\n",
    "            self.train_dataset = load_from_disk(self.local_tdata_cache_path)\n",
    "\n",
    "        return super().setup(stage)\n",
    "\n",
    "    def _tokenize(self, data):\n",
    "        inputs = data['text']\n",
    "        return self.tokenizer.batch_encode_plus(inputs, return_tensors=\"pt\", padding=True, return_length=True, max_length=self.max_length)\n",
    "\n",
    "    \n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        if os.path.exists(self.local_tokenized_cache_path):\n",
    "            train_dataset = load_from_disk(self.local_tokenized_cache_path)\n",
    "        else:\n",
    "            train_dataset = self.train_dataset.map(self._tokenize, batched=True, batch_size=self.batch_size).select_columns([\"input_ids\", \"attention_mask\"])\n",
    "            train_dataset.save_to_disk(self.local_tokenized_cache_path)\n",
    "        l = len(train_dataset)\n",
    "        print(range(self.batch_size * self.resume_pos, l))\n",
    "        train_dataset = train_dataset.select(range(self.batch_size * self.resume_pos, l))\n",
    "        return data.DataLoader(train_dataset.with_format(type=\"torch\"), num_workers=self.num_proc, batch_size=self.batch_size)\n",
    "    \n",
    "    #.skip(self.batch_size * self.resume_pos)\n",
    "    def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        val_dataset:Dataset = self.val_dataset.map(self._tokenize, batched=True,  batch_size=self.batch_size).select_columns([\"input_ids\", \"attention_mask\"])\n",
    "        return data.DataLoader(val_dataset.with_format(type=\"torch\"), num_workers=self.num_proc, batch_size=self.batch_size)\n",
    "    \n",
    "    def test_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        test_dataset = self.dataset[\"test\"].map(self._tokenize, batched=True, batch_size=self.batch_size).select_columns([\"input_ids\", \"attention_mask\"])\n",
    "        return data.DataLoader(test_dataset.with_format(type=\"torch\"), num_workers=self.num_proc, batch_size=self.batch_size)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "block_size = 512\n",
    "batch_size = 10\n",
    "\n",
    "data_module = HuggingFaceCollectionModule(tokenizer, paths=['wikimedia/wikisource', \"togethercomputer/RedPajama-Data-1T-Sample\"],\n",
    "                                              subsets=[\n",
    "                                                  ['20231201.en'],\n",
    "                                                  [None]\n",
    "                                              ],\n",
    "                                              max_length=block_size, \n",
    "                                              batch_size=batch_size, \n",
    "                                              num_proc=15, \n",
    "                                              train_size=0.99)\n",
    "data_module.prepare_data()\n",
    "data_module.setup(\"fit\")\n",
    "train_data = data_module.train_dataloader()\n",
    "val_data = data_module.val_dataloader()\n",
    "test_data = data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_tokens_count(loader):\n",
    "    overall_token_count = 0\n",
    "    for data in tqdm(loader):\n",
    "        token_lengths: torch.Tensor = data['input_ids']\n",
    "        overall_token_count += token_lengths.numel()\n",
    "\n",
    "    return overall_token_count\n",
    "\n",
    "print(f\"training : {get_tokens_count(train_data) / 1e9} billion tokens\")\n",
    "print(f\"validation : {get_tokens_count(val_data) / 1e9} billion tokens\")\n",
    "print(f\"test : {get_tokens_count(test_data) / 1e9} billion tokens\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before Training\n",
    "- 0.4B tokens의 training data는 Chinchilla Optimal의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "linear = torch.nn.Linear(768, 50000)\n",
    "embedding = torch.nn.Embedding(50000, 768, _weight=linear.weight)\n",
    "embedding.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from os.path import isfile, join\n",
    "\n",
    "def get_last_modified_file(directory):\n",
    "    files = glob.glob(os.path.join(directory, '*'))\n",
    "    last_modified_time = 0\n",
    "    last_modified_file = ''\n",
    "\n",
    "    for file in files:\n",
    "        if isfile(file):\n",
    "            current_time = os.path.getmtime(file)\n",
    "            if current_time > last_modified_time:\n",
    "                last_modified_time = current_time\n",
    "                last_modified_file = file\n",
    "\n",
    "    return last_modified_file\n",
    "\n",
    "\n",
    "\n",
    "get_last_modified_file('checkpoints')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ToyGPT\n",
    "\n",
    "last_ckpt_file = get_last_modified_file('checkpoints')\n",
    "model = ToyGPT.load_from_checkpoint(last_ckpt_file)\n",
    "model.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ToyGPTModelConfig\n",
    "import json\n",
    "\n",
    "configs = [\n",
    "    ToyGPTModelConfig(name='toyGPT.small', n_layer=12, n_head=8, block_size=512, n_embed=768).dict(),\n",
    "    ToyGPTModelConfig(name='toyGPT.midium', n_layer=16, n_head=8, block_size=512, n_embed=1024).dict(),\n",
    "    ToyGPTModelConfig(name='toyGPT.large', n_layer=24, n_head=16, block_size=512, n_embed=2048).dict(),\n",
    "]\n",
    "\n",
    "\n",
    "with open('config.json', 'w+') as fp:\n",
    "    json.dump(configs, fp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLM on GPT???\n",
    "\n",
    "- 특수 Token 몇개를 추가하여 제한된 MLM을 통해 추가적인 학습이 가능할지?\n",
    "\n",
    "### Data Augmentation\n",
    "\n",
    "- MLM Task를 구분하기 위한 특수 토큰 <mlm> </mlm> 추가\n",
    "- Input Sequence \"I love you\" 가 주어졌을 때\n",
    "- <mlm>I <msk> you</mlm>love 와 같은 방식으로 causal modeling과 유사하게 mlm을 처리\n",
    "- 단, loss function을 구할 때 flatten하여 모든 step에 대한 loss를 구했다면\n",
    "- mlm의 경우 마지막 token에 대한 loss만 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List,Dict\n",
    "from transformers import PreTrainedTokenizer, GPT2Tokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def generate_choices(list_of_indices:List[int], choice_fraction):\n",
    "    # Shuffle the list to ensure randomness\n",
    "    k = len(list_of_indices) * choice_fraction\n",
    "    if k < 1:\n",
    "        raise ValueError(\"Choice fraction too small to create any set.\")\n",
    "    list_of_indices = set(list_of_indices)\n",
    "\n",
    "    k = int(k)\n",
    "    set_of_choices = []\n",
    "    while len(list_of_indices) > k:\n",
    "        choices = set(random.sample(list(list_of_indices), k=k))\n",
    "        list_of_indices = list_of_indices.difference(choices)\n",
    "        set_of_choices.append(choices)\n",
    "    if len(list_of_indices) > 0:\n",
    "        set_of_choices.append(list_of_indices)\n",
    "    return set_of_choices\n",
    "\n",
    "class MLMAugmentation:\n",
    "\n",
    "    def __init__(self,  datasets: List[Dataset], tokenizer: PreTrainedTokenizer, colunm_selection:str, masking_fraction:float=0.15) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.datasets = datasets\n",
    "        self.column_selection = colunm_selection\n",
    "        self.masking_fraction = masking_fraction\n",
    "\n",
    "\n",
    "    def __call__(self, *args: Any, **kwds: Any) -> Any:\n",
    "        for dataset in self.datasets:\n",
    "            for data in dataset:\n",
    "                sample = data[self.column_selection]\n",
    "                result = self.tokenizer.encode_plus(f\"<cls>{sample}<sep>\", return_tensors=\"pt\", return_attention_mask=False)\n",
    "                input_ids:torch.Tensor = result['input_ids']\n",
    "                choices = generate_choices(list(range(1, input_ids.size(-1) - 1)), self.masking_fraction)\n",
    "                label:torch.Tensor = input_ids.clone().squeeze(0)\n",
    "                input_ids = input_ids.expand((len(choices), input_ids.size(-1)))\n",
    "                for i in range(len(choices)):\n",
    "                    input_ids[i, list(choices[i])] = self.tokenizer.mask_token_id\n",
    "                    result =  {\"input\": input_ids[i], \"label\": label}\n",
    "                    yield result\n",
    "    \n",
    "class CLMAugmentation:\n",
    "\n",
    "    def __init__(self, datasets: List[Dataset], tokenizer: PreTrainedTokenizer, colunm_selection:str) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.datasets = datasets\n",
    "        self.column_selection = colunm_selection\n",
    "\n",
    "    def __call__(self, *args: Any, **kwds: Any) -> Any:\n",
    "\n",
    "        for dataset in self.datasets:\n",
    "            for data in dataset:\n",
    "                sample = data[self.column_selection]\n",
    "                result = self.tokenizer.encode_plus(f\"<|startoftext|>{sample}<|endoftext|>\", return_tensors=\"pt\", return_attention_mask=False)\n",
    "                input_ids:torch.Tensor = result[\"input_ids\"].squeeze(0)\n",
    "                yield {\"input\": input_ids[:-1], \"label\": input_ids[1:]}\n",
    "                \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fritzprix/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Downloading readme: 100%|██████████| 11.6k/11.6k [00:00<00:00, 37.4MB/s]\n",
      "Downloading data: 100%|██████████| 44.3M/44.3M [00:05<00:00, 8.29MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:05<00:00,  5.35s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 2096.10it/s]\n",
      "Setting num_proc from 15 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 51760 examples [00:00, 149697.41 examples/s]\n",
      "Map (num_proc=15):  34%|███▍      | 17804/51760 [00:03<00:04, 7056.40 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1029 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Map (num_proc=15): 100%|██████████| 51760/51760 [00:08<00:00, 6400.19 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<data.CLMAugmentation object at 0x7fc56bc5c100>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting num_proc from 15 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 52460 examples [00:17, 2947.14 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input', 'label'],\n",
      "    num_rows: 52460\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (15/15 shards): 100%|██████████| 47214/47214 [00:00<00:00, 138971.06 examples/s]\n",
      "Saving the dataset (15/15 shards): 100%|██████████| 5246/5246 [00:00<00:00, 15955.49 examples/s]\n",
      "Saving the dataset (15/15 shards): 100%|██████████| 4721/4721 [00:00<00:00, 14042.11 examples/s]\n",
      "Saving the dataset (15/15 shards): 100%|██████████| 42493/42493 [00:00<00:00, 113930.87 examples/s]\n",
      "Map (num_proc=15):   0%|          | 0/51760 [00:00<?, ? examples/s]"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "import torch\n",
    "import lightning as L\n",
    "\n",
    "from transformers import PreTrainedTokenizer, GPT2Tokenizer\n",
    "from data import DataModuleGroup\n",
    "from model import ToyGPTMLM\n",
    "\n",
    "\n",
    "def get_tokenizer() -> PreTrainedTokenizer:\n",
    "    \n",
    "    tokenizer: PreTrainedTokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    \n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"<pad>\", \n",
    "                                  \"mask_token\": \"<msk>\",\n",
    "                                  \"cls_token\": \"<cls>\",\n",
    "                                  \"sep_token\": \"<sep>\",\n",
    "                                  \"bos_token\":\"<|startoftext|>\", \n",
    "                                  \"eos_token\":\"<|endoftext|>\",}) # special \n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "\n",
    "def get_device() -> Any:\n",
    "    # will use GPU whenever it's available\n",
    "    return torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "tokenizer = get_tokenizer()\n",
    "device = get_device()\n",
    "batch_size = 4\n",
    "block_size = 512\n",
    "\n",
    "\n",
    "from data import HFCollectionDataModule\n",
    "\n",
    "clm_dataset = HFCollectionDataModule(tokenizer, \n",
    "                                            paths=['yahma/alpaca-cleaned'], columns=[\"output\"],\n",
    "                                            subsets=[[None]], max_length=block_size, batch_size=1)\n",
    "\n",
    "mlm_dataset = HFCollectionDataModule(tokenizer, \n",
    "                                            paths=['yahma/alpaca-cleaned'], columns=[\"output\"],\n",
    "                                            pretrain_type='MLM',\n",
    "                                            subsets=[[None]], max_length=block_size, batch_size=1)\n",
    "\n",
    "data = DataModuleGroup([clm_dataset, mlm_dataset], [\"clm\", \"mlm\"], batch_size=batch_size)\n",
    "\n",
    "\n",
    "trainer = L.Trainer(max_epochs=1)\n",
    "\n",
    "model = ToyGPTMLM(vocab_size=len(tokenizer), name='toygpt_mlm', batch=batch_size, block_size=block_size, n_embed=768, n_head=8, n_layer=12, mask_token_id=tokenizer.mask_token_id, pad_token_id=tokenizer.pad_token_id, device=device)\n",
    "\n",
    "trainer.fit(model, datamodule=data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor of random integers, for example, between 0 and 9, with a size of your choice\n",
    "a = torch.randint(0, 10, (10,))\n",
    "\n",
    "# Find indices of even numbers\n",
    "even_indices = torch.nonzero(a % 2 == 0).squeeze()\n",
    "\n",
    "# even_indices now contains the indices of even numbers in tensor 'a'\n",
    "print(\"Tensor:\", a)\n",
    "print(\"Indices of even numbers:\", even_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "logits = torch.rand((10, 32, 64))\n",
    "target = torch.randint(0, 63, (10, 32))\n",
    "\n",
    "mask_token_id = 30\n",
    "mask = target == mask_token_id\n",
    "print(torch.nonzero(mask == True))\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "mask\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
